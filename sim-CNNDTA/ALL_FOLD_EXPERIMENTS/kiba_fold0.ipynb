{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "921f00a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import json,pickle,math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f9ad5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(open('../kiba_all_pairs.csv','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "adcec19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_3_folds={}\n",
    "for i in [0,1,2]:\n",
    "    file_name = 'fold' +str(i)\n",
    "\n",
    "    temp = open('../data/kiba/KIBA_3_FOLDS/' + file_name +'.pkl', 'rb')\n",
    "    new_df = pd.read_pickle(temp)\n",
    "    all_3_folds.update({file_name:new_df})\n",
    "    temp.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6caf7aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_3_folds['fold2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "073a9b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_davis_test_train(test_fold_number,all_3_folds):\n",
    "    \n",
    "    test_set = pd.DataFrame(columns = full_df.columns)\n",
    "    train_set = pd.DataFrame(columns= full_df.columns)\n",
    "    for i in [0,1,2]:\n",
    "        fold_name = 'fold' + str(i) \n",
    "        df = all_3_folds[fold_name]\n",
    "\n",
    "        if str(i) == test_fold_number:\n",
    "            test_set = df.copy()\n",
    "\n",
    "        if str(i) != test_fold_number:\n",
    "            train_set = pd.concat([train_set, df.copy()], ignore_index=True)\n",
    "\n",
    "                \n",
    "    return train_set, test_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa4c7e1",
   "metadata": {},
   "source": [
    "# Create train test split on these 3 folds\n",
    "## fold_number is the id of fold. For example, test = fold0, train = fold 1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7651bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_number = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c15d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = create_davis_test_train(test_fold_number=fold_number, all_3_folds=all_3_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8249d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[['SMILES','Target Sequence','Label']]\n",
    "train = train[['SMILES','Target Sequence','Label']]\n",
    "\n",
    "# train =train.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65999a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fa07af",
   "metadata": {},
   "source": [
    "# Creating similarity matrices for this fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "00d70d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "from rdkit.Chem import AllChem as Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.DataStructs import FingerprintSimilarity as fs\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "from Bio import pairwise2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d68ec791",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = list(set(list(train['Target Sequence'])))\n",
    "train_smiles = list(set(list(train['SMILES'])))\n",
    "\n",
    "def computeLigandSimilarity(smiles):\n",
    "    fingerprints = {}\n",
    "    for smile in smiles:\n",
    "        mol = AllChem.MolFromSmiles(smile)\n",
    "        if mol == None:\n",
    "            mol = AllChem.MolFromSmiles(smile, sanitize=False)\n",
    "        fp = FingerprintMols.FingerprintMol(mol)\n",
    "        fingerprints[smile] = fp\n",
    "    \n",
    "    n = len(smiles)\n",
    "    sims = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1):\n",
    "            fpi = fingerprints[smiles[i]]\n",
    "            fpj = fingerprints[smiles[j]]\n",
    "            sim = fs(fpi, fpj)\n",
    "            sims[i, j] = sims[j, i] = sim\n",
    "    return sims\n",
    "\n",
    "def computeProteinSimilarity(targets):\n",
    "    n = len(targets)\n",
    "    mat = np.zeros((n,n))\n",
    "    mat_i = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        seq = targets[i]\n",
    "        s = pairwise2.align.localxx(seq,seq, score_only=True)\n",
    "        mat_i[i] = s\n",
    "        \n",
    "    for i in range(n):\n",
    "        print(i)\n",
    "        for j in range(n):\n",
    "            if mat[i][j] == 0 :\n",
    "                s1 = targets[i]\n",
    "                s2 = targets[j]\n",
    "                sw_ij = pairwise2.align.localxx(s1,s2,score_only=True)\n",
    "                normalized_score = sw_ij /math.sqrt(mat_i[i]*mat_i[j])\n",
    "                mat[i][j] = mat[j][i] = normalized_score\n",
    "    \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0cfbb7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ligand_similarity_matrix = computeLigandSimilarity(train_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a66ea1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2060, 2060)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(ligand_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "79257470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ligand_similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "43d5c331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n"
     ]
    }
   ],
   "source": [
    "print(len(train_targets))\n",
    "protein_similarity_matrix = computeProteinSimilarity(train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b7e3e105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 140)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(protein_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4d62dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSM = ligand_similarity_matrix\n",
    "PSM = protein_similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a61cd5d",
   "metadata": {},
   "source": [
    "# Creating similarity matrcies for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d26b930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_targets = list(set(list(test['Target Sequence'])))\n",
    "test_smiles = list(set(list(test['SMILES'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "874c3826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 140)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_PSM = np.zeros((len(test_targets), len(train_targets)))\n",
    "np.shape(test_PSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1550c589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1e210c92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n"
     ]
    }
   ],
   "source": [
    "s_train_PSM = np.zeros(len(train_targets))\n",
    "s_test_PSM = np.zeros(len(test_targets))\n",
    "\n",
    "for i in range(len(train_targets)):\n",
    "    seq = train_targets[i]\n",
    "    s_train_PSM[i] = pairwise2.align.localxx(seq,seq, score_only=True)\n",
    "    \n",
    "for i in range(len(test_targets)):\n",
    "    seq = test_targets[i]\n",
    "    s_test_PSM[i] = pairwise2.align.localxx(seq,seq, score_only=True)\n",
    "    \n",
    "for i in range(len(test_targets)):\n",
    "    print(i)\n",
    "    for j in range(len(train_targets)):\n",
    "        seq1 = test_targets[i]\n",
    "        seq2 = train_targets[j]\n",
    "        s_ij=pairwise2.align.localxx(seq1, seq2, score_only=True)\n",
    "        N_S = s_ij / math.sqrt(s_train_PSM[j] * s_test_PSM[i])\n",
    "        test_PSM[i][j] = N_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3d270df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1867, 2060)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_LSM = np.zeros((len(test_smiles), len(train_smiles)))\n",
    "np.shape(test_LSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "10e53950",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_smi_fp=[]\n",
    "train_smi_fp = []\n",
    "\n",
    "for smi in test_smiles:\n",
    "    mol1 = AllChem.MolFromSmiles(smi)\n",
    "    if mol1 == None:\n",
    "        mol1= AllChem.MolFromSmiles(smi, sanitize=False)\n",
    "    fp1 = FingerprintMols.FingerprintMol(mol1)\n",
    "    test_smi_fp.append(fp1)\n",
    "\n",
    "for smi in train_smiles:\n",
    "    mol1 = AllChem.MolFromSmiles(smi)\n",
    "    if mol1 == None:\n",
    "        mol1= AllChem.MolFromSmiles(smi, sanitize=False)\n",
    "    fp1 = FingerprintMols.FingerprintMol(mol1)\n",
    "    train_smi_fp.append(fp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "242c7bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_smi_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "741c987f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_smiles)):\n",
    "    print(i)\n",
    "    for j in range(len(train_smiles)):\n",
    "        smi1 = test_smiles[i]\n",
    "        smi2 = train_smiles[j]\n",
    "        \n",
    "        test_LSM[i][j] = fs(test_smi_fp[i], train_smi_fp[j])\n",
    "#         if i==j:\n",
    "#             print(i)\n",
    "#             print(test_LSM[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c9565d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.2812983 , 0.36468984, ..., 0.34758995, 0.30672926,\n",
       "        0.32313433],\n",
       "       [0.2812983 , 1.        , 0.30551627, ..., 0.29166667, 0.26377295,\n",
       "        0.30825243],\n",
       "       [0.34575163, 0.29897611, 0.404375  , ..., 0.40605296, 0.30716724,\n",
       "        0.38030096],\n",
       "       ...,\n",
       "       [0.31189948, 0.26484375, 0.36699164, ..., 0.39198856, 0.25558122,\n",
       "        0.27781872],\n",
       "       [0.35151934, 0.28826896, 0.35673624, ..., 0.34939759, 0.30337886,\n",
       "        0.30874317],\n",
       "       [0.32313433, 0.30825243, 0.34869326, ..., 0.33884298, 0.28368794,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "test_LSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "610a43ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_smiles[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8bac2b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_smiles[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cd940ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "# Hyper parameters\n",
    "num_epochs = 20\n",
    "# num_classes = 10\n",
    "batch_size = 24\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a7c3b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, smiles,smi_index ,targets, target_index, LSM,PSM,transform=None):\n",
    "        self.df = dataframe\n",
    "#         self.root_dir = root_dir\n",
    "        self.smiles =smiles\n",
    "        self.targets = targets\n",
    "        self.LSM = LSM\n",
    "        self.PSM = PSM\n",
    "        self.transform = transform\n",
    "        self.smi_index=smi_index\n",
    "        self.target_index=target_index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        smi = self.df.iloc[idx]['SMILES']\n",
    "        seq = self.df.iloc[idx]['Target Sequence']\n",
    "#         s_i = self.smiles.index(smi)\n",
    "#         t_i = self.targets.index(seq)\n",
    "        s_i = self.smi_index[smi]\n",
    "        t_i = self.target_index[seq]\n",
    "        \n",
    "        ki=self.LSM[s_i]\n",
    "        kj=self.PSM[t_i]\n",
    "        \n",
    "        ki_x_kj = np.outer(ki,kj)\n",
    "        ki_x_kj = torch.tensor([ki_x_kj])\n",
    "        output = {'outer_product': ki_x_kj , 'Label':self.df.iloc[idx]['Label']}\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "12269397",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index_smi={}\n",
    "for i in test_smiles:\n",
    "    test_index_smi[i]=test_smiles.index(i)\n",
    "train_index_smi={}\n",
    "for i in train_smiles:\n",
    "    train_index_smi[i]=train_smiles.index(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5e7f5497",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index_seq={}\n",
    "for i in test_targets:\n",
    "    test_index_seq[i]=test_targets.index(i)\n",
    "\n",
    "train_index_seq={}\n",
    "for i in train_targets:\n",
    "    train_index_seq[i]=train_targets.index(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27e2802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "df6a835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = custom_dataset(dataframe=train, smiles=train_smiles, smi_index=train_index_smi, targets = train_targets, target_index=train_index_seq, LSM=LSM,PSM=PSM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6f4d26a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = custom_dataset(dataframe=test, smiles=test_smiles,smi_index=test_index_smi, targets = test_targets, target_index=test_index_seq, LSM=test_LSM,PSM=test_PSM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e3f85e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 68117/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "38043780",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader= torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader= torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "85c0ecab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118296\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader)*batch_size +  len(test_loader)*batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "05f52951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in test_loader:\n",
    "#     a = i['outer_product']\n",
    "#     b= i['Label']\n",
    "#     break\n",
    "# # print(a)\n",
    "# conv1 = nn.Conv2d(1,32,5).double()\n",
    "# pool = nn.MaxPool2d(2,2).double()\n",
    "# conv2 = nn.Conv2d(32,18,3).double()\n",
    "# fc1 = nn.Linear(18*513*33, 128).double()\n",
    "# fc2 = nn.Linear(128,1).double()\n",
    "# dropout = nn.Dropout(0.1).double()\n",
    "# x= conv1(a)\n",
    "# print(x.shape)\n",
    "# x = pool(x)\n",
    "# print(x.shape)\n",
    "# x= conv2(x)\n",
    "# print(x.shape)\n",
    "# x = pool(x)\n",
    "# print(x.shape)\n",
    "# x = x.view(-1,18*513*33)\n",
    "# print(x.shape)\n",
    "# x = dropout(x)\n",
    "# print(x.shape)\n",
    "# x = fc1(x)\n",
    "# print(x.shape)\n",
    "# x = fc2(x)\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b064796e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d809accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,32, 5).double()\n",
    "        self.pool1 = nn.MaxPool2d(2,2).double()\n",
    "        self.conv2 = nn.Conv2d(32,18,3).double()\n",
    "        self.pool2 = nn.MaxPool2d(2,2).double()\n",
    "        self.fc1 = nn.Linear(18*513*33, 128).double()\n",
    "        self.fc2 = nn.Linear(128,1).double()\n",
    "        self.dropout = nn.Dropout(0.1).double()\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1,18*513*33)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c6690d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "081f070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "081225a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y,f):\n",
    "    rmse = math.sqrt(((y - f)**2).mean(axis=0))\n",
    "    return rmse\n",
    "def mse(y,f):\n",
    "    mse = ((y - f)**2).mean(axis=0)\n",
    "    return mse\n",
    "def pearson(y,f):\n",
    "    rp = np.corrcoef(y, f)[0,1]\n",
    "    return rp\n",
    "from lifelines.utils import concordance_index\n",
    "def ci(y,f):\n",
    "    return concordance_index(y,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7474bdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicting(model, device, test_loader):\n",
    "    model.eval()\n",
    "    total_preds = np.array([])\n",
    "    total_labels = np.array([])\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        c=0\n",
    "        for i in test_loader:\n",
    "            print(c)\n",
    "            c=c+1\n",
    "            if(c>100):\n",
    "                break\n",
    "            images = i['outer_product']\n",
    "            labels = i['Label']\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images) \n",
    "            outputs = outputs.cpu().detach().numpy().flatten()\n",
    "            labels =labels.cpu().detach().numpy().flatten()\n",
    "            total_preds = np.concatenate([total_preds, outputs])\n",
    "            total_labels = np.concatenate([total_labels, labels])\n",
    "    model.train()   \n",
    "    return total_labels, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ca05b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = 'best_sim-CNN-DTA_kiba_fold' + fold_number + \"NEW\"+ '.model'\n",
    "result_file_name = 'best_result_sim-CNNDTA_kiba_fold'+fold_number + \"NEW\"+ '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "32906a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9e0cb3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_loader:\n",
    "#     a = i['outer_product']\n",
    "#     b= i['Label']\n",
    "#     o = model(a.to(device))\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9da2f2a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# G,P = predicting(model, device, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "00ffb125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d12932d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [1/2839], Loss: 148.7818\n",
      "Epoch [1/20], Step [2/2839], Loss: 50.5890\n",
      "Epoch [1/20], Step [3/2839], Loss: 1.2489\n",
      "Epoch [1/20], Step [4/2839], Loss: 10.4442\n",
      "Epoch [1/20], Step [5/2839], Loss: 9.2366\n",
      "Epoch [1/20], Step [6/2839], Loss: 1.8275\n",
      "Epoch [1/20], Step [7/2839], Loss: 1.3997\n",
      "Epoch [1/20], Step [8/2839], Loss: 4.0028\n",
      "Epoch [1/20], Step [9/2839], Loss: 3.8050\n",
      "Epoch [1/20], Step [10/2839], Loss: 1.4119\n",
      "Epoch [1/20], Step [11/2839], Loss: 1.4588\n",
      "Epoch [1/20], Step [12/2839], Loss: 3.2799\n",
      "Epoch [1/20], Step [13/2839], Loss: 2.2418\n",
      "Epoch [1/20], Step [14/2839], Loss: 1.3534\n",
      "Epoch [1/20], Step [15/2839], Loss: 0.6803\n",
      "Epoch [1/20], Step [16/2839], Loss: 1.9371\n",
      "Epoch [1/20], Step [17/2839], Loss: 1.1957\n",
      "Epoch [1/20], Step [18/2839], Loss: 1.3460\n",
      "Epoch [1/20], Step [19/2839], Loss: 0.8396\n",
      "Epoch [1/20], Step [20/2839], Loss: 1.5649\n",
      "Epoch [1/20], Step [21/2839], Loss: 1.2979\n",
      "Epoch [1/20], Step [22/2839], Loss: 1.6117\n",
      "Epoch [1/20], Step [23/2839], Loss: 0.6143\n",
      "Epoch [1/20], Step [24/2839], Loss: 0.8890\n",
      "Epoch [1/20], Step [25/2839], Loss: 1.0648\n",
      "Epoch [1/20], Step [26/2839], Loss: 0.6720\n",
      "Epoch [1/20], Step [27/2839], Loss: 0.8194\n",
      "Epoch [1/20], Step [28/2839], Loss: 0.4876\n",
      "Epoch [1/20], Step [29/2839], Loss: 1.4550\n",
      "Epoch [1/20], Step [30/2839], Loss: 0.4552\n",
      "Epoch [1/20], Step [31/2839], Loss: 0.2110\n",
      "Epoch [1/20], Step [32/2839], Loss: 0.3509\n",
      "Epoch [1/20], Step [33/2839], Loss: 1.1397\n",
      "Epoch [1/20], Step [34/2839], Loss: 0.5520\n",
      "Epoch [1/20], Step [35/2839], Loss: 0.5098\n",
      "Epoch [1/20], Step [36/2839], Loss: 0.3741\n",
      "Epoch [1/20], Step [37/2839], Loss: 0.7739\n",
      "Epoch [1/20], Step [38/2839], Loss: 0.7567\n",
      "Epoch [1/20], Step [39/2839], Loss: 0.5437\n",
      "Epoch [1/20], Step [40/2839], Loss: 1.0700\n",
      "Epoch [1/20], Step [41/2839], Loss: 1.0672\n",
      "Epoch [1/20], Step [42/2839], Loss: 1.4703\n",
      "Epoch [1/20], Step [43/2839], Loss: 0.8883\n",
      "Epoch [1/20], Step [44/2839], Loss: 0.4950\n",
      "Epoch [1/20], Step [45/2839], Loss: 0.8501\n",
      "Epoch [1/20], Step [46/2839], Loss: 0.8035\n",
      "Epoch [1/20], Step [47/2839], Loss: 0.9274\n",
      "Epoch [1/20], Step [48/2839], Loss: 0.4445\n",
      "Epoch [1/20], Step [49/2839], Loss: 0.9488\n",
      "Epoch [1/20], Step [50/2839], Loss: 0.8394\n",
      "Epoch [1/20], Step [51/2839], Loss: 0.5657\n",
      "Epoch [1/20], Step [52/2839], Loss: 0.9570\n",
      "Epoch [1/20], Step [53/2839], Loss: 0.9671\n",
      "Epoch [1/20], Step [54/2839], Loss: 0.5436\n",
      "Epoch [1/20], Step [55/2839], Loss: 1.0203\n",
      "Epoch [1/20], Step [56/2839], Loss: 0.8832\n",
      "Epoch [1/20], Step [57/2839], Loss: 0.7702\n",
      "Epoch [1/20], Step [58/2839], Loss: 0.6651\n",
      "Epoch [1/20], Step [59/2839], Loss: 0.3718\n",
      "Epoch [1/20], Step [60/2839], Loss: 0.7080\n",
      "Epoch [1/20], Step [61/2839], Loss: 0.7713\n",
      "Epoch [1/20], Step [62/2839], Loss: 0.6817\n",
      "Epoch [1/20], Step [63/2839], Loss: 0.9455\n",
      "Epoch [1/20], Step [64/2839], Loss: 0.5082\n",
      "Epoch [1/20], Step [65/2839], Loss: 0.5216\n",
      "Epoch [1/20], Step [66/2839], Loss: 1.0965\n",
      "Epoch [1/20], Step [67/2839], Loss: 0.4471\n",
      "Epoch [1/20], Step [68/2839], Loss: 0.7631\n",
      "Epoch [1/20], Step [69/2839], Loss: 0.7171\n",
      "Epoch [1/20], Step [70/2839], Loss: 0.3535\n",
      "Epoch [1/20], Step [71/2839], Loss: 0.8808\n",
      "Epoch [1/20], Step [72/2839], Loss: 0.6211\n",
      "Epoch [1/20], Step [73/2839], Loss: 0.7399\n",
      "Epoch [1/20], Step [74/2839], Loss: 1.0220\n",
      "Epoch [1/20], Step [75/2839], Loss: 0.8209\n",
      "Epoch [1/20], Step [76/2839], Loss: 0.7248\n",
      "Epoch [1/20], Step [77/2839], Loss: 1.1549\n",
      "Epoch [1/20], Step [78/2839], Loss: 0.2513\n",
      "Epoch [1/20], Step [79/2839], Loss: 1.0056\n",
      "Epoch [1/20], Step [80/2839], Loss: 0.6991\n",
      "Epoch [1/20], Step [81/2839], Loss: 0.4286\n",
      "Epoch [1/20], Step [82/2839], Loss: 0.5711\n",
      "Epoch [1/20], Step [83/2839], Loss: 0.7940\n",
      "Epoch [1/20], Step [84/2839], Loss: 0.8309\n",
      "Epoch [1/20], Step [85/2839], Loss: 0.5202\n",
      "Epoch [1/20], Step [86/2839], Loss: 0.2349\n",
      "Epoch [1/20], Step [87/2839], Loss: 0.9751\n",
      "Epoch [1/20], Step [88/2839], Loss: 1.0634\n",
      "Epoch [1/20], Step [89/2839], Loss: 0.4713\n",
      "Epoch [1/20], Step [90/2839], Loss: 1.0158\n",
      "Epoch [1/20], Step [91/2839], Loss: 1.0520\n",
      "Epoch [1/20], Step [92/2839], Loss: 0.5238\n",
      "Epoch [1/20], Step [93/2839], Loss: 1.1419\n",
      "Epoch [1/20], Step [94/2839], Loss: 0.6454\n",
      "Epoch [1/20], Step [95/2839], Loss: 1.5216\n",
      "Epoch [1/20], Step [96/2839], Loss: 0.6875\n",
      "Epoch [1/20], Step [97/2839], Loss: 0.6304\n",
      "Epoch [1/20], Step [98/2839], Loss: 0.6944\n",
      "Epoch [1/20], Step [99/2839], Loss: 0.5470\n",
      "Epoch [1/20], Step [100/2839], Loss: 0.3489\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[0.8289204273470735, 0.6871090748732549, 0.21404910165146585, 0.5763213044890839]\n",
      "rmse improved at epoch  1 ; best_mse,best_ci,best_r: 0.6871090748732549 0.5763213044890839 0.21404910165146585\n",
      "Epoch [2/20], Step [1/2839], Loss: 0.7231\n",
      "Epoch [2/20], Step [2/2839], Loss: 0.3821\n",
      "Epoch [2/20], Step [3/2839], Loss: 0.7259\n",
      "Epoch [2/20], Step [4/2839], Loss: 0.4219\n",
      "Epoch [2/20], Step [5/2839], Loss: 0.4183\n",
      "Epoch [2/20], Step [6/2839], Loss: 0.7737\n",
      "Epoch [2/20], Step [7/2839], Loss: 1.2948\n",
      "Epoch [2/20], Step [8/2839], Loss: 0.3918\n",
      "Epoch [2/20], Step [9/2839], Loss: 1.0401\n",
      "Epoch [2/20], Step [10/2839], Loss: 0.7661\n",
      "Epoch [2/20], Step [11/2839], Loss: 0.4436\n",
      "Epoch [2/20], Step [12/2839], Loss: 0.5758\n",
      "Epoch [2/20], Step [13/2839], Loss: 1.0036\n",
      "Epoch [2/20], Step [14/2839], Loss: 0.4999\n",
      "Epoch [2/20], Step [15/2839], Loss: 0.7229\n",
      "Epoch [2/20], Step [16/2839], Loss: 0.2094\n",
      "Epoch [2/20], Step [17/2839], Loss: 0.8952\n",
      "Epoch [2/20], Step [18/2839], Loss: 0.4565\n",
      "Epoch [2/20], Step [19/2839], Loss: 0.7718\n",
      "Epoch [2/20], Step [20/2839], Loss: 1.5881\n",
      "Epoch [2/20], Step [21/2839], Loss: 0.8797\n",
      "Epoch [2/20], Step [22/2839], Loss: 0.9996\n",
      "Epoch [2/20], Step [23/2839], Loss: 1.1108\n",
      "Epoch [2/20], Step [24/2839], Loss: 1.1952\n",
      "Epoch [2/20], Step [25/2839], Loss: 0.4065\n",
      "Epoch [2/20], Step [26/2839], Loss: 0.9775\n",
      "Epoch [2/20], Step [27/2839], Loss: 1.3076\n",
      "Epoch [2/20], Step [28/2839], Loss: 0.5640\n",
      "Epoch [2/20], Step [29/2839], Loss: 1.7911\n",
      "Epoch [2/20], Step [30/2839], Loss: 0.8648\n",
      "Epoch [2/20], Step [31/2839], Loss: 0.3481\n",
      "Epoch [2/20], Step [32/2839], Loss: 2.0091\n",
      "Epoch [2/20], Step [33/2839], Loss: 1.5536\n",
      "Epoch [2/20], Step [34/2839], Loss: 0.4204\n",
      "Epoch [2/20], Step [35/2839], Loss: 0.9099\n",
      "Epoch [2/20], Step [36/2839], Loss: 1.1441\n",
      "Epoch [2/20], Step [37/2839], Loss: 0.7695\n",
      "Epoch [2/20], Step [38/2839], Loss: 1.2653\n",
      "Epoch [2/20], Step [39/2839], Loss: 0.5410\n",
      "Epoch [2/20], Step [40/2839], Loss: 0.8480\n",
      "Epoch [2/20], Step [41/2839], Loss: 0.7922\n",
      "Epoch [2/20], Step [42/2839], Loss: 0.9496\n",
      "Epoch [2/20], Step [43/2839], Loss: 0.8026\n",
      "Epoch [2/20], Step [44/2839], Loss: 0.7060\n",
      "Epoch [2/20], Step [45/2839], Loss: 0.4294\n",
      "Epoch [2/20], Step [46/2839], Loss: 0.1420\n",
      "Epoch [2/20], Step [47/2839], Loss: 1.0537\n",
      "Epoch [2/20], Step [48/2839], Loss: 0.6504\n",
      "Epoch [2/20], Step [49/2839], Loss: 0.4565\n",
      "Epoch [2/20], Step [50/2839], Loss: 0.2988\n",
      "Epoch [2/20], Step [51/2839], Loss: 0.5055\n",
      "Epoch [2/20], Step [52/2839], Loss: 0.7472\n",
      "Epoch [2/20], Step [53/2839], Loss: 1.0887\n",
      "Epoch [2/20], Step [54/2839], Loss: 0.5933\n",
      "Epoch [2/20], Step [55/2839], Loss: 0.7013\n",
      "Epoch [2/20], Step [56/2839], Loss: 0.6823\n",
      "Epoch [2/20], Step [57/2839], Loss: 0.7450\n",
      "Epoch [2/20], Step [58/2839], Loss: 0.2527\n",
      "Epoch [2/20], Step [59/2839], Loss: 0.4349\n",
      "Epoch [2/20], Step [60/2839], Loss: 0.7719\n",
      "Epoch [2/20], Step [61/2839], Loss: 1.7769\n",
      "Epoch [2/20], Step [62/2839], Loss: 1.2981\n",
      "Epoch [2/20], Step [63/2839], Loss: 0.6526\n",
      "Epoch [2/20], Step [64/2839], Loss: 0.4129\n",
      "Epoch [2/20], Step [65/2839], Loss: 0.9773\n",
      "Epoch [2/20], Step [66/2839], Loss: 2.2560\n",
      "Epoch [2/20], Step [67/2839], Loss: 1.0717\n",
      "Epoch [2/20], Step [68/2839], Loss: 1.3825\n",
      "Epoch [2/20], Step [69/2839], Loss: 2.0059\n",
      "Epoch [2/20], Step [70/2839], Loss: 0.4548\n",
      "Epoch [2/20], Step [71/2839], Loss: 1.9188\n",
      "Epoch [2/20], Step [72/2839], Loss: 3.1651\n",
      "Epoch [2/20], Step [73/2839], Loss: 0.7948\n",
      "Epoch [2/20], Step [74/2839], Loss: 1.2036\n",
      "Epoch [2/20], Step [75/2839], Loss: 1.7810\n",
      "Epoch [2/20], Step [76/2839], Loss: 1.1115\n",
      "Epoch [2/20], Step [77/2839], Loss: 1.0837\n",
      "Epoch [2/20], Step [78/2839], Loss: 1.3770\n",
      "Epoch [2/20], Step [79/2839], Loss: 0.9407\n",
      "Epoch [2/20], Step [80/2839], Loss: 0.9413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Step [81/2839], Loss: 0.6739\n",
      "Epoch [2/20], Step [82/2839], Loss: 0.7514\n",
      "Epoch [2/20], Step [83/2839], Loss: 1.5576\n",
      "Epoch [2/20], Step [84/2839], Loss: 0.8383\n",
      "Epoch [2/20], Step [85/2839], Loss: 0.5686\n",
      "Epoch [2/20], Step [86/2839], Loss: 0.4469\n",
      "Epoch [2/20], Step [87/2839], Loss: 0.4008\n",
      "Epoch [2/20], Step [88/2839], Loss: 0.4348\n",
      "Epoch [2/20], Step [89/2839], Loss: 0.4503\n",
      "Epoch [2/20], Step [90/2839], Loss: 0.7225\n",
      "Epoch [2/20], Step [91/2839], Loss: 0.9589\n",
      "Epoch [2/20], Step [92/2839], Loss: 1.0722\n",
      "Epoch [2/20], Step [93/2839], Loss: 0.7872\n",
      "Epoch [2/20], Step [94/2839], Loss: 1.0511\n",
      "Epoch [2/20], Step [95/2839], Loss: 0.5604\n",
      "Epoch [2/20], Step [96/2839], Loss: 0.7770\n",
      "Epoch [2/20], Step [97/2839], Loss: 0.6975\n",
      "Epoch [2/20], Step [98/2839], Loss: 0.4644\n",
      "Epoch [2/20], Step [99/2839], Loss: 1.2653\n",
      "Epoch [2/20], Step [100/2839], Loss: 1.1162\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[1.016192693619, 1.0326475905646386, 0.19002312350225325, 0.5685865947010748]\n",
      "Epoch [3/20], Step [1/2839], Loss: 1.3839\n",
      "Epoch [3/20], Step [2/2839], Loss: 1.0456\n",
      "Epoch [3/20], Step [3/2839], Loss: 1.0094\n",
      "Epoch [3/20], Step [4/2839], Loss: 0.5665\n",
      "Epoch [3/20], Step [5/2839], Loss: 0.7493\n",
      "Epoch [3/20], Step [6/2839], Loss: 0.5564\n",
      "Epoch [3/20], Step [7/2839], Loss: 0.6829\n",
      "Epoch [3/20], Step [8/2839], Loss: 0.3215\n",
      "Epoch [3/20], Step [9/2839], Loss: 0.9002\n",
      "Epoch [3/20], Step [10/2839], Loss: 0.5956\n",
      "Epoch [3/20], Step [11/2839], Loss: 1.1575\n",
      "Epoch [3/20], Step [12/2839], Loss: 1.1579\n",
      "Epoch [3/20], Step [13/2839], Loss: 1.3049\n",
      "Epoch [3/20], Step [14/2839], Loss: 0.5731\n",
      "Epoch [3/20], Step [15/2839], Loss: 0.6587\n",
      "Epoch [3/20], Step [16/2839], Loss: 2.4280\n",
      "Epoch [3/20], Step [17/2839], Loss: 1.0743\n",
      "Epoch [3/20], Step [18/2839], Loss: 1.1622\n",
      "Epoch [3/20], Step [19/2839], Loss: 1.0911\n",
      "Epoch [3/20], Step [20/2839], Loss: 0.5413\n",
      "Epoch [3/20], Step [21/2839], Loss: 1.3792\n",
      "Epoch [3/20], Step [22/2839], Loss: 1.2440\n",
      "Epoch [3/20], Step [23/2839], Loss: 0.8910\n",
      "Epoch [3/20], Step [24/2839], Loss: 0.8472\n",
      "Epoch [3/20], Step [25/2839], Loss: 1.6456\n",
      "Epoch [3/20], Step [26/2839], Loss: 0.5149\n",
      "Epoch [3/20], Step [27/2839], Loss: 0.5444\n",
      "Epoch [3/20], Step [28/2839], Loss: 1.0572\n",
      "Epoch [3/20], Step [29/2839], Loss: 1.3555\n",
      "Epoch [3/20], Step [30/2839], Loss: 1.5931\n",
      "Epoch [3/20], Step [31/2839], Loss: 1.3571\n",
      "Epoch [3/20], Step [32/2839], Loss: 0.8877\n",
      "Epoch [3/20], Step [33/2839], Loss: 0.3444\n",
      "Epoch [3/20], Step [34/2839], Loss: 0.9205\n",
      "Epoch [3/20], Step [35/2839], Loss: 1.5391\n",
      "Epoch [3/20], Step [36/2839], Loss: 0.6449\n",
      "Epoch [3/20], Step [37/2839], Loss: 0.4831\n",
      "Epoch [3/20], Step [38/2839], Loss: 0.8115\n",
      "Epoch [3/20], Step [39/2839], Loss: 0.8309\n",
      "Epoch [3/20], Step [40/2839], Loss: 1.5147\n",
      "Epoch [3/20], Step [41/2839], Loss: 0.3498\n",
      "Epoch [3/20], Step [42/2839], Loss: 1.1063\n",
      "Epoch [3/20], Step [43/2839], Loss: 0.6125\n",
      "Epoch [3/20], Step [44/2839], Loss: 0.3414\n",
      "Epoch [3/20], Step [45/2839], Loss: 0.1997\n",
      "Epoch [3/20], Step [46/2839], Loss: 1.0879\n",
      "Epoch [3/20], Step [47/2839], Loss: 0.6095\n",
      "Epoch [3/20], Step [48/2839], Loss: 0.4654\n",
      "Epoch [3/20], Step [49/2839], Loss: 0.5627\n",
      "Epoch [3/20], Step [50/2839], Loss: 0.9923\n",
      "Epoch [3/20], Step [51/2839], Loss: 0.9358\n",
      "Epoch [3/20], Step [52/2839], Loss: 0.5593\n",
      "Epoch [3/20], Step [53/2839], Loss: 0.5751\n",
      "Epoch [3/20], Step [54/2839], Loss: 0.8230\n",
      "Epoch [3/20], Step [55/2839], Loss: 1.2420\n",
      "Epoch [3/20], Step [56/2839], Loss: 0.8468\n",
      "Epoch [3/20], Step [57/2839], Loss: 1.0650\n",
      "Epoch [3/20], Step [58/2839], Loss: 1.2808\n",
      "Epoch [3/20], Step [59/2839], Loss: 0.6547\n",
      "Epoch [3/20], Step [60/2839], Loss: 0.9573\n",
      "Epoch [3/20], Step [61/2839], Loss: 0.8856\n",
      "Epoch [3/20], Step [62/2839], Loss: 0.5029\n",
      "Epoch [3/20], Step [63/2839], Loss: 0.1322\n",
      "Epoch [3/20], Step [64/2839], Loss: 0.4529\n",
      "Epoch [3/20], Step [65/2839], Loss: 0.3704\n",
      "Epoch [3/20], Step [66/2839], Loss: 0.6770\n",
      "Epoch [3/20], Step [67/2839], Loss: 0.7421\n",
      "Epoch [3/20], Step [68/2839], Loss: 0.4324\n",
      "Epoch [3/20], Step [69/2839], Loss: 0.8933\n",
      "Epoch [3/20], Step [70/2839], Loss: 0.9107\n",
      "Epoch [3/20], Step [71/2839], Loss: 0.8793\n",
      "Epoch [3/20], Step [72/2839], Loss: 0.7245\n",
      "Epoch [3/20], Step [73/2839], Loss: 1.0168\n",
      "Epoch [3/20], Step [74/2839], Loss: 0.7328\n",
      "Epoch [3/20], Step [75/2839], Loss: 0.7126\n",
      "Epoch [3/20], Step [76/2839], Loss: 0.7101\n",
      "Epoch [3/20], Step [77/2839], Loss: 0.3694\n",
      "Epoch [3/20], Step [78/2839], Loss: 0.6587\n",
      "Epoch [3/20], Step [79/2839], Loss: 0.1757\n",
      "Epoch [3/20], Step [80/2839], Loss: 0.6700\n",
      "Epoch [3/20], Step [81/2839], Loss: 0.2407\n",
      "Epoch [3/20], Step [82/2839], Loss: 0.3428\n",
      "Epoch [3/20], Step [83/2839], Loss: 0.4415\n",
      "Epoch [3/20], Step [84/2839], Loss: 0.2649\n",
      "Epoch [3/20], Step [85/2839], Loss: 0.5893\n",
      "Epoch [3/20], Step [86/2839], Loss: 1.2459\n",
      "Epoch [3/20], Step [87/2839], Loss: 0.5199\n",
      "Epoch [3/20], Step [88/2839], Loss: 0.5100\n",
      "Epoch [3/20], Step [89/2839], Loss: 0.5356\n",
      "Epoch [3/20], Step [90/2839], Loss: 0.3877\n",
      "Epoch [3/20], Step [91/2839], Loss: 0.3348\n",
      "Epoch [3/20], Step [92/2839], Loss: 0.9375\n",
      "Epoch [3/20], Step [93/2839], Loss: 0.8446\n",
      "Epoch [3/20], Step [94/2839], Loss: 0.4860\n",
      "Epoch [3/20], Step [95/2839], Loss: 0.6954\n",
      "Epoch [3/20], Step [96/2839], Loss: 1.0669\n",
      "Epoch [3/20], Step [97/2839], Loss: 1.0297\n",
      "Epoch [3/20], Step [98/2839], Loss: 0.5444\n",
      "Epoch [3/20], Step [99/2839], Loss: 0.5911\n",
      "Epoch [3/20], Step [100/2839], Loss: 0.9149\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[0.8499146218926878, 0.7223548645069905, 0.22509571870335274, 0.5810468025861855]\n",
      "Epoch [4/20], Step [1/2839], Loss: 0.7950\n",
      "Epoch [4/20], Step [2/2839], Loss: 0.5958\n",
      "Epoch [4/20], Step [3/2839], Loss: 0.6201\n",
      "Epoch [4/20], Step [4/2839], Loss: 0.7240\n",
      "Epoch [4/20], Step [5/2839], Loss: 0.5151\n",
      "Epoch [4/20], Step [6/2839], Loss: 0.9854\n",
      "Epoch [4/20], Step [7/2839], Loss: 0.9440\n",
      "Epoch [4/20], Step [8/2839], Loss: 1.1376\n",
      "Epoch [4/20], Step [9/2839], Loss: 1.3313\n",
      "Epoch [4/20], Step [10/2839], Loss: 1.2895\n",
      "Epoch [4/20], Step [11/2839], Loss: 0.2112\n",
      "Epoch [4/20], Step [12/2839], Loss: 0.8601\n",
      "Epoch [4/20], Step [13/2839], Loss: 0.6209\n",
      "Epoch [4/20], Step [14/2839], Loss: 0.7530\n",
      "Epoch [4/20], Step [15/2839], Loss: 0.9329\n",
      "Epoch [4/20], Step [16/2839], Loss: 0.6887\n",
      "Epoch [4/20], Step [17/2839], Loss: 0.7060\n",
      "Epoch [4/20], Step [18/2839], Loss: 0.3019\n",
      "Epoch [4/20], Step [19/2839], Loss: 1.4214\n",
      "Epoch [4/20], Step [20/2839], Loss: 0.5787\n",
      "Epoch [4/20], Step [21/2839], Loss: 0.5957\n",
      "Epoch [4/20], Step [22/2839], Loss: 1.4601\n",
      "Epoch [4/20], Step [23/2839], Loss: 0.4981\n",
      "Epoch [4/20], Step [24/2839], Loss: 0.9142\n",
      "Epoch [4/20], Step [25/2839], Loss: 0.8800\n",
      "Epoch [4/20], Step [26/2839], Loss: 0.5534\n",
      "Epoch [4/20], Step [27/2839], Loss: 0.6754\n",
      "Epoch [4/20], Step [28/2839], Loss: 0.7979\n",
      "Epoch [4/20], Step [29/2839], Loss: 1.2283\n",
      "Epoch [4/20], Step [30/2839], Loss: 0.3058\n",
      "Epoch [4/20], Step [31/2839], Loss: 0.2704\n",
      "Epoch [4/20], Step [32/2839], Loss: 0.4766\n",
      "Epoch [4/20], Step [33/2839], Loss: 1.0043\n",
      "Epoch [4/20], Step [34/2839], Loss: 0.1831\n",
      "Epoch [4/20], Step [35/2839], Loss: 0.4449\n",
      "Epoch [4/20], Step [36/2839], Loss: 1.4058\n",
      "Epoch [4/20], Step [37/2839], Loss: 1.3190\n",
      "Epoch [4/20], Step [38/2839], Loss: 0.5265\n",
      "Epoch [4/20], Step [39/2839], Loss: 1.1749\n",
      "Epoch [4/20], Step [40/2839], Loss: 0.6736\n",
      "Epoch [4/20], Step [41/2839], Loss: 0.9985\n",
      "Epoch [4/20], Step [42/2839], Loss: 0.7641\n",
      "Epoch [4/20], Step [43/2839], Loss: 0.4044\n",
      "Epoch [4/20], Step [44/2839], Loss: 0.3669\n",
      "Epoch [4/20], Step [45/2839], Loss: 0.3522\n",
      "Epoch [4/20], Step [46/2839], Loss: 0.9866\n",
      "Epoch [4/20], Step [47/2839], Loss: 1.2736\n",
      "Epoch [4/20], Step [48/2839], Loss: 0.6778\n",
      "Epoch [4/20], Step [49/2839], Loss: 1.2320\n",
      "Epoch [4/20], Step [50/2839], Loss: 0.3237\n",
      "Epoch [4/20], Step [51/2839], Loss: 0.4684\n",
      "Epoch [4/20], Step [52/2839], Loss: 0.2380\n",
      "Epoch [4/20], Step [53/2839], Loss: 0.5357\n",
      "Epoch [4/20], Step [54/2839], Loss: 0.6110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Step [55/2839], Loss: 0.6671\n",
      "Epoch [4/20], Step [56/2839], Loss: 0.4938\n",
      "Epoch [4/20], Step [57/2839], Loss: 0.3611\n",
      "Epoch [4/20], Step [58/2839], Loss: 0.7844\n",
      "Epoch [4/20], Step [59/2839], Loss: 0.3818\n",
      "Epoch [4/20], Step [60/2839], Loss: 0.7372\n",
      "Epoch [4/20], Step [61/2839], Loss: 0.6673\n",
      "Epoch [4/20], Step [62/2839], Loss: 0.7119\n",
      "Epoch [4/20], Step [63/2839], Loss: 1.0946\n",
      "Epoch [4/20], Step [64/2839], Loss: 1.0447\n",
      "Epoch [4/20], Step [65/2839], Loss: 1.1792\n",
      "Epoch [4/20], Step [66/2839], Loss: 1.1789\n",
      "Epoch [4/20], Step [67/2839], Loss: 0.7353\n",
      "Epoch [4/20], Step [68/2839], Loss: 0.3729\n",
      "Epoch [4/20], Step [69/2839], Loss: 0.7063\n",
      "Epoch [4/20], Step [70/2839], Loss: 0.4294\n",
      "Epoch [4/20], Step [71/2839], Loss: 0.3868\n",
      "Epoch [4/20], Step [72/2839], Loss: 0.8579\n",
      "Epoch [4/20], Step [73/2839], Loss: 0.4596\n",
      "Epoch [4/20], Step [74/2839], Loss: 0.5121\n",
      "Epoch [4/20], Step [75/2839], Loss: 0.8457\n",
      "Epoch [4/20], Step [76/2839], Loss: 1.1241\n",
      "Epoch [4/20], Step [77/2839], Loss: 0.4664\n",
      "Epoch [4/20], Step [78/2839], Loss: 1.2770\n",
      "Epoch [4/20], Step [79/2839], Loss: 1.0004\n",
      "Epoch [4/20], Step [80/2839], Loss: 0.4390\n",
      "Epoch [4/20], Step [81/2839], Loss: 1.0030\n",
      "Epoch [4/20], Step [82/2839], Loss: 0.6536\n",
      "Epoch [4/20], Step [83/2839], Loss: 0.5222\n",
      "Epoch [4/20], Step [84/2839], Loss: 0.8805\n",
      "Epoch [4/20], Step [85/2839], Loss: 0.2532\n",
      "Epoch [4/20], Step [86/2839], Loss: 0.1939\n",
      "Epoch [4/20], Step [87/2839], Loss: 1.4573\n",
      "Epoch [4/20], Step [88/2839], Loss: 0.3293\n",
      "Epoch [4/20], Step [89/2839], Loss: 0.4066\n",
      "Epoch [4/20], Step [90/2839], Loss: 0.5704\n",
      "Epoch [4/20], Step [91/2839], Loss: 0.3443\n",
      "Epoch [4/20], Step [92/2839], Loss: 1.5764\n",
      "Epoch [4/20], Step [93/2839], Loss: 0.6678\n",
      "Epoch [4/20], Step [94/2839], Loss: 1.4658\n",
      "Epoch [4/20], Step [95/2839], Loss: 1.0892\n",
      "Epoch [4/20], Step [96/2839], Loss: 0.6732\n",
      "Epoch [4/20], Step [97/2839], Loss: 0.5297\n",
      "Epoch [4/20], Step [98/2839], Loss: 1.2727\n",
      "Epoch [4/20], Step [99/2839], Loss: 0.7626\n",
      "Epoch [4/20], Step [100/2839], Loss: 0.4176\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[0.8166057685523564, 0.6668449812329847, 0.2338638364840334, 0.5914985937775935]\n",
      "rmse improved at epoch  4 ; best_mse,best_ci,best_r: 0.6668449812329847 0.5914985937775935 0.2338638364840334\n",
      "Epoch [5/20], Step [1/2839], Loss: 0.5907\n",
      "Epoch [5/20], Step [2/2839], Loss: 1.3156\n",
      "Epoch [5/20], Step [3/2839], Loss: 1.1041\n",
      "Epoch [5/20], Step [4/2839], Loss: 0.3946\n",
      "Epoch [5/20], Step [5/2839], Loss: 0.6339\n",
      "Epoch [5/20], Step [6/2839], Loss: 0.4067\n",
      "Epoch [5/20], Step [7/2839], Loss: 0.3144\n",
      "Epoch [5/20], Step [8/2839], Loss: 0.9595\n",
      "Epoch [5/20], Step [9/2839], Loss: 0.9596\n",
      "Epoch [5/20], Step [10/2839], Loss: 0.8263\n",
      "Epoch [5/20], Step [11/2839], Loss: 0.4510\n",
      "Epoch [5/20], Step [12/2839], Loss: 0.5212\n",
      "Epoch [5/20], Step [13/2839], Loss: 0.5029\n",
      "Epoch [5/20], Step [14/2839], Loss: 0.7705\n",
      "Epoch [5/20], Step [15/2839], Loss: 0.7523\n",
      "Epoch [5/20], Step [16/2839], Loss: 0.8280\n",
      "Epoch [5/20], Step [17/2839], Loss: 1.1954\n",
      "Epoch [5/20], Step [18/2839], Loss: 0.4432\n",
      "Epoch [5/20], Step [19/2839], Loss: 0.7940\n",
      "Epoch [5/20], Step [20/2839], Loss: 1.0968\n",
      "Epoch [5/20], Step [21/2839], Loss: 0.5748\n",
      "Epoch [5/20], Step [22/2839], Loss: 0.6076\n",
      "Epoch [5/20], Step [23/2839], Loss: 0.5868\n",
      "Epoch [5/20], Step [24/2839], Loss: 0.2985\n",
      "Epoch [5/20], Step [25/2839], Loss: 0.4865\n",
      "Epoch [5/20], Step [26/2839], Loss: 0.5841\n",
      "Epoch [5/20], Step [27/2839], Loss: 0.8578\n",
      "Epoch [5/20], Step [28/2839], Loss: 0.7452\n",
      "Epoch [5/20], Step [29/2839], Loss: 0.7856\n",
      "Epoch [5/20], Step [30/2839], Loss: 0.3502\n",
      "Epoch [5/20], Step [31/2839], Loss: 0.8592\n",
      "Epoch [5/20], Step [32/2839], Loss: 0.7424\n",
      "Epoch [5/20], Step [33/2839], Loss: 0.9187\n",
      "Epoch [5/20], Step [34/2839], Loss: 0.4521\n",
      "Epoch [5/20], Step [35/2839], Loss: 0.8486\n",
      "Epoch [5/20], Step [36/2839], Loss: 0.6042\n",
      "Epoch [5/20], Step [37/2839], Loss: 0.3465\n",
      "Epoch [5/20], Step [38/2839], Loss: 1.9298\n",
      "Epoch [5/20], Step [39/2839], Loss: 1.3704\n",
      "Epoch [5/20], Step [40/2839], Loss: 1.6692\n",
      "Epoch [5/20], Step [41/2839], Loss: 1.9116\n",
      "Epoch [5/20], Step [42/2839], Loss: 0.7098\n",
      "Epoch [5/20], Step [43/2839], Loss: 1.3039\n",
      "Epoch [5/20], Step [44/2839], Loss: 1.0206\n",
      "Epoch [5/20], Step [45/2839], Loss: 0.7321\n",
      "Epoch [5/20], Step [46/2839], Loss: 1.2721\n",
      "Epoch [5/20], Step [47/2839], Loss: 1.2719\n",
      "Epoch [5/20], Step [48/2839], Loss: 0.4016\n",
      "Epoch [5/20], Step [49/2839], Loss: 1.1987\n",
      "Epoch [5/20], Step [50/2839], Loss: 0.8498\n",
      "Epoch [5/20], Step [51/2839], Loss: 0.7342\n",
      "Epoch [5/20], Step [52/2839], Loss: 1.1863\n",
      "Epoch [5/20], Step [53/2839], Loss: 1.0924\n",
      "Epoch [5/20], Step [54/2839], Loss: 0.5061\n",
      "Epoch [5/20], Step [55/2839], Loss: 0.4461\n",
      "Epoch [5/20], Step [56/2839], Loss: 0.4477\n",
      "Epoch [5/20], Step [57/2839], Loss: 0.5042\n",
      "Epoch [5/20], Step [58/2839], Loss: 0.5634\n",
      "Epoch [5/20], Step [59/2839], Loss: 0.6042\n",
      "Epoch [5/20], Step [60/2839], Loss: 1.5358\n",
      "Epoch [5/20], Step [61/2839], Loss: 1.1831\n",
      "Epoch [5/20], Step [62/2839], Loss: 1.2584\n",
      "Epoch [5/20], Step [63/2839], Loss: 0.5224\n",
      "Epoch [5/20], Step [64/2839], Loss: 0.8040\n",
      "Epoch [5/20], Step [65/2839], Loss: 0.7871\n",
      "Epoch [5/20], Step [66/2839], Loss: 0.3736\n",
      "Epoch [5/20], Step [67/2839], Loss: 0.8668\n",
      "Epoch [5/20], Step [68/2839], Loss: 0.9718\n",
      "Epoch [5/20], Step [69/2839], Loss: 0.7581\n",
      "Epoch [5/20], Step [70/2839], Loss: 0.5966\n",
      "Epoch [5/20], Step [71/2839], Loss: 0.3930\n",
      "Epoch [5/20], Step [72/2839], Loss: 0.5385\n",
      "Epoch [5/20], Step [73/2839], Loss: 0.4706\n",
      "Epoch [5/20], Step [74/2839], Loss: 0.3557\n",
      "Epoch [5/20], Step [75/2839], Loss: 0.7336\n",
      "Epoch [5/20], Step [76/2839], Loss: 0.9749\n",
      "Epoch [5/20], Step [77/2839], Loss: 0.8177\n",
      "Epoch [5/20], Step [78/2839], Loss: 0.4612\n",
      "Epoch [5/20], Step [79/2839], Loss: 0.8816\n",
      "Epoch [5/20], Step [80/2839], Loss: 0.2257\n",
      "Epoch [5/20], Step [81/2839], Loss: 0.4816\n",
      "Epoch [5/20], Step [82/2839], Loss: 0.7409\n",
      "Epoch [5/20], Step [83/2839], Loss: 0.7894\n",
      "Epoch [5/20], Step [84/2839], Loss: 0.4019\n",
      "Epoch [5/20], Step [85/2839], Loss: 0.4804\n",
      "Epoch [5/20], Step [86/2839], Loss: 1.0496\n",
      "Epoch [5/20], Step [87/2839], Loss: 0.0999\n",
      "Epoch [5/20], Step [88/2839], Loss: 1.3586\n",
      "Epoch [5/20], Step [89/2839], Loss: 1.3358\n",
      "Epoch [5/20], Step [90/2839], Loss: 0.9757\n",
      "Epoch [5/20], Step [91/2839], Loss: 1.1585\n",
      "Epoch [5/20], Step [92/2839], Loss: 2.3046\n",
      "Epoch [5/20], Step [93/2839], Loss: 1.5402\n",
      "Epoch [5/20], Step [94/2839], Loss: 1.5346\n",
      "Epoch [5/20], Step [95/2839], Loss: 1.7704\n",
      "Epoch [5/20], Step [96/2839], Loss: 1.1469\n",
      "Epoch [5/20], Step [97/2839], Loss: 1.6874\n",
      "Epoch [5/20], Step [98/2839], Loss: 0.8508\n",
      "Epoch [5/20], Step [99/2839], Loss: 0.1995\n",
      "Epoch [5/20], Step [100/2839], Loss: 0.4053\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[0.9535716260874016, 0.9092988460789714, 0.3033575347341751, 0.6182662952207109]\n",
      "Epoch [6/20], Step [1/2839], Loss: 1.1430\n",
      "Epoch [6/20], Step [2/2839], Loss: 1.0668\n",
      "Epoch [6/20], Step [3/2839], Loss: 0.3362\n",
      "Epoch [6/20], Step [4/2839], Loss: 0.5047\n",
      "Epoch [6/20], Step [5/2839], Loss: 0.5151\n",
      "Epoch [6/20], Step [6/2839], Loss: 0.6297\n",
      "Epoch [6/20], Step [7/2839], Loss: 1.0461\n",
      "Epoch [6/20], Step [8/2839], Loss: 0.7479\n",
      "Epoch [6/20], Step [9/2839], Loss: 0.4060\n",
      "Epoch [6/20], Step [10/2839], Loss: 0.4068\n",
      "Epoch [6/20], Step [11/2839], Loss: 1.7598\n",
      "Epoch [6/20], Step [12/2839], Loss: 0.6211\n",
      "Epoch [6/20], Step [13/2839], Loss: 0.9098\n",
      "Epoch [6/20], Step [14/2839], Loss: 1.1859\n",
      "Epoch [6/20], Step [15/2839], Loss: 0.5207\n",
      "Epoch [6/20], Step [16/2839], Loss: 0.9283\n",
      "Epoch [6/20], Step [17/2839], Loss: 1.1316\n",
      "Epoch [6/20], Step [18/2839], Loss: 0.3656\n",
      "Epoch [6/20], Step [19/2839], Loss: 1.8443\n",
      "Epoch [6/20], Step [20/2839], Loss: 1.9257\n",
      "Epoch [6/20], Step [21/2839], Loss: 1.4662\n",
      "Epoch [6/20], Step [22/2839], Loss: 0.4200\n",
      "Epoch [6/20], Step [23/2839], Loss: 1.6486\n",
      "Epoch [6/20], Step [24/2839], Loss: 1.5873\n",
      "Epoch [6/20], Step [25/2839], Loss: 0.9703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Step [26/2839], Loss: 1.4679\n",
      "Epoch [6/20], Step [27/2839], Loss: 1.0944\n",
      "Epoch [6/20], Step [28/2839], Loss: 0.4898\n",
      "Epoch [6/20], Step [29/2839], Loss: 1.4588\n",
      "Epoch [6/20], Step [30/2839], Loss: 2.5016\n",
      "Epoch [6/20], Step [31/2839], Loss: 0.9748\n",
      "Epoch [6/20], Step [32/2839], Loss: 2.1162\n",
      "Epoch [6/20], Step [33/2839], Loss: 0.9404\n",
      "Epoch [6/20], Step [34/2839], Loss: 1.1732\n",
      "Epoch [6/20], Step [35/2839], Loss: 1.6730\n",
      "Epoch [6/20], Step [36/2839], Loss: 0.5384\n",
      "Epoch [6/20], Step [37/2839], Loss: 1.1092\n",
      "Epoch [6/20], Step [38/2839], Loss: 1.1617\n",
      "Epoch [6/20], Step [39/2839], Loss: 0.6792\n",
      "Epoch [6/20], Step [40/2839], Loss: 0.7689\n",
      "Epoch [6/20], Step [41/2839], Loss: 0.7664\n",
      "Epoch [6/20], Step [42/2839], Loss: 1.5068\n",
      "Epoch [6/20], Step [43/2839], Loss: 0.9723\n",
      "Epoch [6/20], Step [44/2839], Loss: 1.5603\n",
      "Epoch [6/20], Step [45/2839], Loss: 0.4523\n",
      "Epoch [6/20], Step [46/2839], Loss: 0.2850\n",
      "Epoch [6/20], Step [47/2839], Loss: 2.0044\n",
      "Epoch [6/20], Step [48/2839], Loss: 0.3103\n",
      "Epoch [6/20], Step [49/2839], Loss: 0.7018\n",
      "Epoch [6/20], Step [50/2839], Loss: 1.1758\n",
      "Epoch [6/20], Step [51/2839], Loss: 0.4734\n",
      "Epoch [6/20], Step [52/2839], Loss: 0.4142\n",
      "Epoch [6/20], Step [53/2839], Loss: 0.6698\n",
      "Epoch [6/20], Step [54/2839], Loss: 0.9798\n",
      "Epoch [6/20], Step [55/2839], Loss: 0.3667\n",
      "Epoch [6/20], Step [56/2839], Loss: 1.0123\n",
      "Epoch [6/20], Step [57/2839], Loss: 1.5436\n",
      "Epoch [6/20], Step [58/2839], Loss: 1.1465\n",
      "Epoch [6/20], Step [59/2839], Loss: 0.3380\n",
      "Epoch [6/20], Step [60/2839], Loss: 2.7740\n",
      "Epoch [6/20], Step [61/2839], Loss: 0.2769\n",
      "Epoch [6/20], Step [62/2839], Loss: 0.8802\n",
      "Epoch [6/20], Step [63/2839], Loss: 0.8239\n",
      "Epoch [6/20], Step [64/2839], Loss: 0.6579\n",
      "Epoch [6/20], Step [65/2839], Loss: 0.3486\n",
      "Epoch [6/20], Step [66/2839], Loss: 0.9913\n",
      "Epoch [6/20], Step [67/2839], Loss: 0.5471\n",
      "Epoch [6/20], Step [68/2839], Loss: 1.1481\n",
      "Epoch [6/20], Step [69/2839], Loss: 0.6101\n",
      "Epoch [6/20], Step [70/2839], Loss: 0.8110\n",
      "Epoch [6/20], Step [71/2839], Loss: 1.3970\n",
      "Epoch [6/20], Step [72/2839], Loss: 1.3033\n",
      "Epoch [6/20], Step [73/2839], Loss: 0.4908\n",
      "Epoch [6/20], Step [74/2839], Loss: 0.8765\n",
      "Epoch [6/20], Step [75/2839], Loss: 0.7760\n",
      "Epoch [6/20], Step [76/2839], Loss: 0.3398\n",
      "Epoch [6/20], Step [77/2839], Loss: 0.6113\n",
      "Epoch [6/20], Step [78/2839], Loss: 0.4610\n",
      "Epoch [6/20], Step [79/2839], Loss: 0.7274\n",
      "Epoch [6/20], Step [80/2839], Loss: 0.2633\n",
      "Epoch [6/20], Step [81/2839], Loss: 0.7376\n",
      "Epoch [6/20], Step [82/2839], Loss: 0.6807\n",
      "Epoch [6/20], Step [83/2839], Loss: 0.2863\n",
      "Epoch [6/20], Step [84/2839], Loss: 0.2846\n",
      "Epoch [6/20], Step [85/2839], Loss: 0.5352\n",
      "Epoch [6/20], Step [86/2839], Loss: 0.3530\n",
      "Epoch [6/20], Step [87/2839], Loss: 0.5933\n",
      "Epoch [6/20], Step [88/2839], Loss: 0.3918\n",
      "Epoch [6/20], Step [89/2839], Loss: 1.0789\n",
      "Epoch [6/20], Step [90/2839], Loss: 1.2123\n",
      "Epoch [6/20], Step [91/2839], Loss: 0.4207\n",
      "Epoch [6/20], Step [92/2839], Loss: 0.2995\n",
      "Epoch [6/20], Step [93/2839], Loss: 0.7141\n",
      "Epoch [6/20], Step [94/2839], Loss: 0.5960\n",
      "Epoch [6/20], Step [95/2839], Loss: 0.6805\n",
      "Epoch [6/20], Step [96/2839], Loss: 1.0016\n",
      "Epoch [6/20], Step [97/2839], Loss: 1.8341\n",
      "Epoch [6/20], Step [98/2839], Loss: 0.5125\n",
      "Epoch [6/20], Step [99/2839], Loss: 1.1592\n",
      "Epoch [6/20], Step [100/2839], Loss: 1.4119\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[0.8611192890822791, 0.7415264300295696, 0.3302259792206853, 0.6250204404005477]\n",
      "Epoch [7/20], Step [1/2839], Loss: 1.1947\n",
      "Epoch [7/20], Step [2/2839], Loss: 1.1186\n",
      "Epoch [7/20], Step [3/2839], Loss: 1.9750\n",
      "Epoch [7/20], Step [4/2839], Loss: 0.8139\n",
      "Epoch [7/20], Step [5/2839], Loss: 1.0079\n",
      "Epoch [7/20], Step [6/2839], Loss: 0.9943\n",
      "Epoch [7/20], Step [7/2839], Loss: 1.1195\n",
      "Epoch [7/20], Step [8/2839], Loss: 1.0261\n",
      "Epoch [7/20], Step [9/2839], Loss: 1.5839\n",
      "Epoch [7/20], Step [10/2839], Loss: 0.5855\n",
      "Epoch [7/20], Step [11/2839], Loss: 0.9989\n",
      "Epoch [7/20], Step [12/2839], Loss: 0.7110\n",
      "Epoch [7/20], Step [13/2839], Loss: 0.9199\n",
      "Epoch [7/20], Step [14/2839], Loss: 0.9761\n",
      "Epoch [7/20], Step [15/2839], Loss: 0.9998\n",
      "Epoch [7/20], Step [16/2839], Loss: 1.1715\n",
      "Epoch [7/20], Step [17/2839], Loss: 0.8527\n",
      "Epoch [7/20], Step [18/2839], Loss: 0.6441\n",
      "Epoch [7/20], Step [19/2839], Loss: 0.6501\n",
      "Epoch [7/20], Step [20/2839], Loss: 1.3618\n",
      "Epoch [7/20], Step [21/2839], Loss: 0.6115\n",
      "Epoch [7/20], Step [22/2839], Loss: 1.0008\n",
      "Epoch [7/20], Step [23/2839], Loss: 1.1049\n",
      "Epoch [7/20], Step [24/2839], Loss: 0.2556\n",
      "Epoch [7/20], Step [25/2839], Loss: 0.8708\n",
      "Epoch [7/20], Step [26/2839], Loss: 0.2334\n",
      "Epoch [7/20], Step [27/2839], Loss: 1.0331\n",
      "Epoch [7/20], Step [28/2839], Loss: 0.6177\n",
      "Epoch [7/20], Step [29/2839], Loss: 0.7472\n",
      "Epoch [7/20], Step [30/2839], Loss: 0.4436\n",
      "Epoch [7/20], Step [31/2839], Loss: 1.1364\n",
      "Epoch [7/20], Step [32/2839], Loss: 1.3048\n",
      "Epoch [7/20], Step [33/2839], Loss: 1.2958\n",
      "Epoch [7/20], Step [34/2839], Loss: 0.6795\n",
      "Epoch [7/20], Step [35/2839], Loss: 1.4273\n",
      "Epoch [7/20], Step [36/2839], Loss: 0.6321\n",
      "Epoch [7/20], Step [37/2839], Loss: 1.1897\n",
      "Epoch [7/20], Step [38/2839], Loss: 0.6706\n",
      "Epoch [7/20], Step [39/2839], Loss: 0.4360\n",
      "Epoch [7/20], Step [40/2839], Loss: 0.7786\n",
      "Epoch [7/20], Step [41/2839], Loss: 0.3310\n",
      "Epoch [7/20], Step [42/2839], Loss: 0.8576\n",
      "Epoch [7/20], Step [43/2839], Loss: 0.8746\n",
      "Epoch [7/20], Step [44/2839], Loss: 0.3790\n",
      "Epoch [7/20], Step [45/2839], Loss: 0.3000\n",
      "Epoch [7/20], Step [46/2839], Loss: 0.4294\n",
      "Epoch [7/20], Step [47/2839], Loss: 0.4072\n",
      "Epoch [7/20], Step [48/2839], Loss: 0.6466\n",
      "Epoch [7/20], Step [49/2839], Loss: 0.2631\n",
      "Epoch [7/20], Step [50/2839], Loss: 0.5188\n",
      "Epoch [7/20], Step [51/2839], Loss: 0.4377\n",
      "Epoch [7/20], Step [52/2839], Loss: 0.2760\n",
      "Epoch [7/20], Step [53/2839], Loss: 0.2926\n",
      "Epoch [7/20], Step [54/2839], Loss: 0.5709\n",
      "Epoch [7/20], Step [55/2839], Loss: 0.9398\n",
      "Epoch [7/20], Step [56/2839], Loss: 1.0891\n",
      "Epoch [7/20], Step [57/2839], Loss: 0.8234\n",
      "Epoch [7/20], Step [58/2839], Loss: 1.5633\n",
      "Epoch [7/20], Step [59/2839], Loss: 0.4572\n",
      "Epoch [7/20], Step [60/2839], Loss: 0.3348\n",
      "Epoch [7/20], Step [61/2839], Loss: 0.6414\n",
      "Epoch [7/20], Step [62/2839], Loss: 0.9748\n",
      "Epoch [7/20], Step [63/2839], Loss: 0.7564\n",
      "Epoch [7/20], Step [64/2839], Loss: 0.7454\n",
      "Epoch [7/20], Step [65/2839], Loss: 0.3689\n",
      "Epoch [7/20], Step [66/2839], Loss: 0.7505\n",
      "Epoch [7/20], Step [67/2839], Loss: 0.5296\n",
      "Epoch [7/20], Step [68/2839], Loss: 0.6389\n",
      "Epoch [7/20], Step [69/2839], Loss: 0.3164\n",
      "Epoch [7/20], Step [70/2839], Loss: 0.4918\n",
      "Epoch [7/20], Step [71/2839], Loss: 0.7738\n",
      "Epoch [7/20], Step [72/2839], Loss: 0.4317\n",
      "Epoch [7/20], Step [73/2839], Loss: 0.5389\n",
      "Epoch [7/20], Step [74/2839], Loss: 0.3289\n",
      "Epoch [7/20], Step [75/2839], Loss: 0.5979\n",
      "Epoch [7/20], Step [76/2839], Loss: 0.3773\n",
      "Epoch [7/20], Step [77/2839], Loss: 0.4069\n",
      "Epoch [7/20], Step [78/2839], Loss: 0.5216\n",
      "Epoch [7/20], Step [79/2839], Loss: 0.6718\n",
      "Epoch [7/20], Step [80/2839], Loss: 0.7722\n",
      "Epoch [7/20], Step [81/2839], Loss: 1.2704\n",
      "Epoch [7/20], Step [82/2839], Loss: 0.3277\n",
      "Epoch [7/20], Step [83/2839], Loss: 0.6142\n",
      "Epoch [7/20], Step [84/2839], Loss: 0.6162\n",
      "Epoch [7/20], Step [85/2839], Loss: 0.3645\n",
      "Epoch [7/20], Step [86/2839], Loss: 0.6273\n",
      "Epoch [7/20], Step [87/2839], Loss: 0.5457\n",
      "Epoch [7/20], Step [88/2839], Loss: 1.1153\n",
      "Epoch [7/20], Step [89/2839], Loss: 0.6208\n",
      "Epoch [7/20], Step [90/2839], Loss: 1.2705\n",
      "Epoch [7/20], Step [91/2839], Loss: 0.4333\n",
      "Epoch [7/20], Step [92/2839], Loss: 0.8463\n",
      "Epoch [7/20], Step [93/2839], Loss: 0.7107\n",
      "Epoch [7/20], Step [94/2839], Loss: 0.5910\n",
      "Epoch [7/20], Step [95/2839], Loss: 0.5736\n",
      "Epoch [7/20], Step [96/2839], Loss: 0.5927\n",
      "Epoch [7/20], Step [97/2839], Loss: 0.6070\n",
      "Epoch [7/20], Step [98/2839], Loss: 0.5780\n",
      "Epoch [7/20], Step [99/2839], Loss: 0.8330\n",
      "Epoch [7/20], Step [100/2839], Loss: 0.4423\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[0.7493270442103858, 0.5614910191850735, 0.3346920282907178, 0.6365305323543078]\n",
      "rmse improved at epoch  7 ; best_mse,best_ci,best_r: 0.5614910191850735 0.6365305323543078 0.3346920282907178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Step [1/2839], Loss: 0.1582\n",
      "Epoch [8/20], Step [2/2839], Loss: 0.4783\n",
      "Epoch [8/20], Step [3/2839], Loss: 0.5714\n",
      "Epoch [8/20], Step [4/2839], Loss: 0.8525\n",
      "Epoch [8/20], Step [5/2839], Loss: 0.2202\n",
      "Epoch [8/20], Step [6/2839], Loss: 0.9310\n",
      "Epoch [8/20], Step [7/2839], Loss: 0.3587\n",
      "Epoch [8/20], Step [8/2839], Loss: 0.5696\n",
      "Epoch [8/20], Step [9/2839], Loss: 0.3710\n",
      "Epoch [8/20], Step [10/2839], Loss: 0.2645\n",
      "Epoch [8/20], Step [11/2839], Loss: 0.8095\n",
      "Epoch [8/20], Step [12/2839], Loss: 0.5291\n",
      "Epoch [8/20], Step [13/2839], Loss: 0.7059\n",
      "Epoch [8/20], Step [14/2839], Loss: 0.9651\n",
      "Epoch [8/20], Step [15/2839], Loss: 0.8489\n",
      "Epoch [8/20], Step [16/2839], Loss: 0.5561\n",
      "Epoch [8/20], Step [17/2839], Loss: 0.5845\n",
      "Epoch [8/20], Step [18/2839], Loss: 1.5284\n",
      "Epoch [8/20], Step [19/2839], Loss: 0.3151\n",
      "Epoch [8/20], Step [20/2839], Loss: 1.0640\n",
      "Epoch [8/20], Step [21/2839], Loss: 0.4668\n",
      "Epoch [8/20], Step [22/2839], Loss: 1.1044\n",
      "Epoch [8/20], Step [23/2839], Loss: 0.3912\n",
      "Epoch [8/20], Step [24/2839], Loss: 0.7124\n",
      "Epoch [8/20], Step [25/2839], Loss: 0.7054\n",
      "Epoch [8/20], Step [26/2839], Loss: 0.7466\n",
      "Epoch [8/20], Step [27/2839], Loss: 1.1330\n",
      "Epoch [8/20], Step [28/2839], Loss: 0.3494\n",
      "Epoch [8/20], Step [29/2839], Loss: 0.3126\n",
      "Epoch [8/20], Step [30/2839], Loss: 1.0934\n",
      "Epoch [8/20], Step [31/2839], Loss: 0.2421\n",
      "Epoch [8/20], Step [32/2839], Loss: 0.6831\n",
      "Epoch [8/20], Step [33/2839], Loss: 0.8229\n",
      "Epoch [8/20], Step [34/2839], Loss: 0.3055\n",
      "Epoch [8/20], Step [35/2839], Loss: 1.1115\n",
      "Epoch [8/20], Step [36/2839], Loss: 0.2218\n",
      "Epoch [8/20], Step [37/2839], Loss: 1.0624\n",
      "Epoch [8/20], Step [38/2839], Loss: 0.5123\n",
      "Epoch [8/20], Step [39/2839], Loss: 0.7674\n",
      "Epoch [8/20], Step [40/2839], Loss: 1.0194\n",
      "Epoch [8/20], Step [41/2839], Loss: 0.4459\n",
      "Epoch [8/20], Step [42/2839], Loss: 1.4698\n",
      "Epoch [8/20], Step [43/2839], Loss: 0.4884\n",
      "Epoch [8/20], Step [44/2839], Loss: 0.5966\n",
      "Epoch [8/20], Step [45/2839], Loss: 1.3892\n",
      "Epoch [8/20], Step [46/2839], Loss: 0.7974\n",
      "Epoch [8/20], Step [47/2839], Loss: 1.0127\n",
      "Epoch [8/20], Step [48/2839], Loss: 1.1579\n",
      "Epoch [8/20], Step [49/2839], Loss: 0.1240\n",
      "Epoch [8/20], Step [50/2839], Loss: 0.5413\n",
      "Epoch [8/20], Step [51/2839], Loss: 0.6591\n",
      "Epoch [8/20], Step [52/2839], Loss: 0.5423\n",
      "Epoch [8/20], Step [53/2839], Loss: 1.5137\n",
      "Epoch [8/20], Step [54/2839], Loss: 0.6459\n",
      "Epoch [8/20], Step [55/2839], Loss: 0.6886\n",
      "Epoch [8/20], Step [56/2839], Loss: 0.4567\n",
      "Epoch [8/20], Step [57/2839], Loss: 0.2694\n",
      "Epoch [8/20], Step [58/2839], Loss: 0.4854\n",
      "Epoch [8/20], Step [59/2839], Loss: 0.9682\n",
      "Epoch [8/20], Step [60/2839], Loss: 0.7853\n",
      "Epoch [8/20], Step [61/2839], Loss: 0.8023\n",
      "Epoch [8/20], Step [62/2839], Loss: 0.6246\n",
      "Epoch [8/20], Step [63/2839], Loss: 1.3565\n",
      "Epoch [8/20], Step [64/2839], Loss: 0.6374\n",
      "Epoch [8/20], Step [65/2839], Loss: 0.8350\n",
      "Epoch [8/20], Step [66/2839], Loss: 0.8647\n",
      "Epoch [8/20], Step [67/2839], Loss: 0.1872\n",
      "Epoch [8/20], Step [68/2839], Loss: 0.5945\n",
      "Epoch [8/20], Step [69/2839], Loss: 1.0228\n",
      "Epoch [8/20], Step [70/2839], Loss: 0.7262\n",
      "Epoch [8/20], Step [71/2839], Loss: 0.3677\n",
      "Epoch [8/20], Step [72/2839], Loss: 0.4591\n",
      "Epoch [8/20], Step [73/2839], Loss: 0.2641\n",
      "Epoch [8/20], Step [74/2839], Loss: 0.7696\n",
      "Epoch [8/20], Step [75/2839], Loss: 0.7549\n",
      "Epoch [8/20], Step [76/2839], Loss: 0.5026\n",
      "Epoch [8/20], Step [77/2839], Loss: 0.6185\n",
      "Epoch [8/20], Step [78/2839], Loss: 0.2753\n",
      "Epoch [8/20], Step [79/2839], Loss: 0.7626\n",
      "Epoch [8/20], Step [80/2839], Loss: 0.6039\n",
      "Epoch [8/20], Step [81/2839], Loss: 0.9078\n",
      "Epoch [8/20], Step [82/2839], Loss: 0.6362\n",
      "Epoch [8/20], Step [83/2839], Loss: 1.1531\n",
      "Epoch [8/20], Step [84/2839], Loss: 1.4795\n",
      "Epoch [8/20], Step [85/2839], Loss: 0.4043\n",
      "Epoch [8/20], Step [86/2839], Loss: 1.0394\n",
      "Epoch [8/20], Step [87/2839], Loss: 1.1574\n",
      "Epoch [8/20], Step [88/2839], Loss: 0.3172\n",
      "Epoch [8/20], Step [89/2839], Loss: 0.4805\n",
      "Epoch [8/20], Step [90/2839], Loss: 0.5314\n",
      "Epoch [8/20], Step [91/2839], Loss: 1.0136\n",
      "Epoch [8/20], Step [92/2839], Loss: 1.3338\n",
      "Epoch [8/20], Step [93/2839], Loss: 1.0520\n",
      "Epoch [8/20], Step [94/2839], Loss: 0.5074\n",
      "Epoch [8/20], Step [95/2839], Loss: 0.4193\n",
      "Epoch [8/20], Step [96/2839], Loss: 0.8812\n",
      "Epoch [8/20], Step [97/2839], Loss: 0.1693\n",
      "Epoch [8/20], Step [98/2839], Loss: 0.4270\n",
      "Epoch [8/20], Step [99/2839], Loss: 0.6214\n",
      "Epoch [8/20], Step [100/2839], Loss: 0.3458\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[0.7757833469276901, 0.6018398013703289, 0.369236510473738, 0.6277853647121627]\n",
      "Epoch [9/20], Step [1/2839], Loss: 0.6172\n",
      "Epoch [9/20], Step [2/2839], Loss: 1.4246\n",
      "Epoch [9/20], Step [3/2839], Loss: 1.7715\n",
      "Epoch [9/20], Step [4/2839], Loss: 0.3412\n",
      "Epoch [9/20], Step [5/2839], Loss: 0.7178\n",
      "Epoch [9/20], Step [6/2839], Loss: 0.5317\n",
      "Epoch [9/20], Step [7/2839], Loss: 0.5618\n",
      "Epoch [9/20], Step [8/2839], Loss: 0.8354\n",
      "Epoch [9/20], Step [9/2839], Loss: 0.2769\n",
      "Epoch [9/20], Step [10/2839], Loss: 0.6899\n",
      "Epoch [9/20], Step [11/2839], Loss: 0.6439\n",
      "Epoch [9/20], Step [12/2839], Loss: 0.6130\n",
      "Epoch [9/20], Step [13/2839], Loss: 0.9675\n",
      "Epoch [9/20], Step [14/2839], Loss: 0.3591\n",
      "Epoch [9/20], Step [15/2839], Loss: 0.5360\n",
      "Epoch [9/20], Step [16/2839], Loss: 0.8190\n",
      "Epoch [9/20], Step [17/2839], Loss: 0.5834\n",
      "Epoch [9/20], Step [18/2839], Loss: 0.8205\n",
      "Epoch [9/20], Step [19/2839], Loss: 0.4221\n",
      "Epoch [9/20], Step [20/2839], Loss: 0.2200\n",
      "Epoch [9/20], Step [21/2839], Loss: 1.0986\n",
      "Epoch [9/20], Step [22/2839], Loss: 0.9920\n",
      "Epoch [9/20], Step [23/2839], Loss: 0.8286\n",
      "Epoch [9/20], Step [24/2839], Loss: 0.7631\n",
      "Epoch [9/20], Step [25/2839], Loss: 0.6266\n",
      "Epoch [9/20], Step [26/2839], Loss: 0.4940\n",
      "Epoch [9/20], Step [27/2839], Loss: 0.6425\n",
      "Epoch [9/20], Step [28/2839], Loss: 0.8885\n",
      "Epoch [9/20], Step [29/2839], Loss: 0.2938\n",
      "Epoch [9/20], Step [30/2839], Loss: 0.4397\n",
      "Epoch [9/20], Step [31/2839], Loss: 0.4819\n",
      "Epoch [9/20], Step [32/2839], Loss: 0.5703\n",
      "Epoch [9/20], Step [33/2839], Loss: 1.5883\n",
      "Epoch [9/20], Step [34/2839], Loss: 0.7821\n",
      "Epoch [9/20], Step [35/2839], Loss: 1.0432\n",
      "Epoch [9/20], Step [36/2839], Loss: 0.7792\n",
      "Epoch [9/20], Step [37/2839], Loss: 0.5396\n",
      "Epoch [9/20], Step [38/2839], Loss: 0.5700\n",
      "Epoch [9/20], Step [39/2839], Loss: 1.5798\n",
      "Epoch [9/20], Step [40/2839], Loss: 0.5066\n",
      "Epoch [9/20], Step [41/2839], Loss: 1.3699\n",
      "Epoch [9/20], Step [42/2839], Loss: 1.6562\n",
      "Epoch [9/20], Step [43/2839], Loss: 0.9466\n",
      "Epoch [9/20], Step [44/2839], Loss: 1.9624\n",
      "Epoch [9/20], Step [45/2839], Loss: 1.0463\n",
      "Epoch [9/20], Step [46/2839], Loss: 0.4649\n",
      "Epoch [9/20], Step [47/2839], Loss: 1.1441\n",
      "Epoch [9/20], Step [48/2839], Loss: 1.4786\n",
      "Epoch [9/20], Step [49/2839], Loss: 1.1220\n",
      "Epoch [9/20], Step [50/2839], Loss: 1.9344\n",
      "Epoch [9/20], Step [51/2839], Loss: 0.7654\n",
      "Epoch [9/20], Step [52/2839], Loss: 0.4595\n",
      "Epoch [9/20], Step [53/2839], Loss: 1.3031\n",
      "Epoch [9/20], Step [54/2839], Loss: 0.6450\n",
      "Epoch [9/20], Step [55/2839], Loss: 0.4403\n",
      "Epoch [9/20], Step [56/2839], Loss: 0.5996\n",
      "Epoch [9/20], Step [57/2839], Loss: 0.7892\n",
      "Epoch [9/20], Step [58/2839], Loss: 0.6186\n",
      "Epoch [9/20], Step [59/2839], Loss: 1.0828\n",
      "Epoch [9/20], Step [60/2839], Loss: 1.0524\n",
      "Epoch [9/20], Step [61/2839], Loss: 0.4716\n",
      "Epoch [9/20], Step [62/2839], Loss: 0.9182\n",
      "Epoch [9/20], Step [63/2839], Loss: 0.4486\n",
      "Epoch [9/20], Step [64/2839], Loss: 0.8663\n",
      "Epoch [9/20], Step [65/2839], Loss: 0.4214\n",
      "Epoch [9/20], Step [66/2839], Loss: 0.6809\n",
      "Epoch [9/20], Step [67/2839], Loss: 0.2247\n",
      "Epoch [9/20], Step [68/2839], Loss: 0.7193\n",
      "Epoch [9/20], Step [69/2839], Loss: 0.5636\n",
      "Epoch [9/20], Step [70/2839], Loss: 0.9973\n",
      "Epoch [9/20], Step [71/2839], Loss: 0.5745\n",
      "Epoch [9/20], Step [72/2839], Loss: 0.5269\n",
      "Epoch [9/20], Step [73/2839], Loss: 5.0218\n",
      "Epoch [9/20], Step [74/2839], Loss: 1.4655\n",
      "Epoch [9/20], Step [75/2839], Loss: 0.4002\n",
      "Epoch [9/20], Step [76/2839], Loss: 0.5584\n",
      "Epoch [9/20], Step [77/2839], Loss: 0.7629\n",
      "Epoch [9/20], Step [78/2839], Loss: 0.9989\n",
      "Epoch [9/20], Step [79/2839], Loss: 0.6907\n",
      "Epoch [9/20], Step [80/2839], Loss: 0.8618\n",
      "Epoch [9/20], Step [81/2839], Loss: 1.2614\n",
      "Epoch [9/20], Step [82/2839], Loss: 0.5699\n",
      "Epoch [9/20], Step [83/2839], Loss: 1.7719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Step [84/2839], Loss: 0.8092\n",
      "Epoch [9/20], Step [85/2839], Loss: 0.5403\n",
      "Epoch [9/20], Step [86/2839], Loss: 1.5851\n",
      "Epoch [9/20], Step [87/2839], Loss: 0.7139\n",
      "Epoch [9/20], Step [88/2839], Loss: 0.7878\n",
      "Epoch [9/20], Step [89/2839], Loss: 1.2440\n",
      "Epoch [9/20], Step [90/2839], Loss: 1.9621\n",
      "Epoch [9/20], Step [91/2839], Loss: 0.5196\n",
      "Epoch [9/20], Step [92/2839], Loss: 0.5737\n",
      "Epoch [9/20], Step [93/2839], Loss: 1.0787\n",
      "Epoch [9/20], Step [94/2839], Loss: 0.4319\n",
      "Epoch [9/20], Step [95/2839], Loss: 1.2514\n",
      "Epoch [9/20], Step [96/2839], Loss: 1.2167\n",
      "Epoch [9/20], Step [97/2839], Loss: 0.7055\n",
      "Epoch [9/20], Step [98/2839], Loss: 0.6870\n",
      "Epoch [9/20], Step [99/2839], Loss: 0.4781\n",
      "Epoch [9/20], Step [100/2839], Loss: 0.6431\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[0.8309866844325787, 0.6905388697042502, 0.3735858604518519, 0.6411500492485449]\n",
      "Epoch [10/20], Step [1/2839], Loss: 0.4184\n",
      "Epoch [10/20], Step [2/2839], Loss: 0.7643\n",
      "Epoch [10/20], Step [3/2839], Loss: 0.8757\n",
      "Epoch [10/20], Step [4/2839], Loss: 3.0419\n",
      "Epoch [10/20], Step [5/2839], Loss: 1.1552\n",
      "Epoch [10/20], Step [6/2839], Loss: 0.5534\n",
      "Epoch [10/20], Step [7/2839], Loss: 0.3836\n",
      "Epoch [10/20], Step [8/2839], Loss: 0.4353\n",
      "Epoch [10/20], Step [9/2839], Loss: 0.3834\n",
      "Epoch [10/20], Step [10/2839], Loss: 0.6056\n",
      "Epoch [10/20], Step [11/2839], Loss: 0.2829\n",
      "Epoch [10/20], Step [12/2839], Loss: 0.7260\n",
      "Epoch [10/20], Step [13/2839], Loss: 0.2394\n",
      "Epoch [10/20], Step [14/2839], Loss: 0.1283\n",
      "Epoch [10/20], Step [15/2839], Loss: 0.4531\n",
      "Epoch [10/20], Step [16/2839], Loss: 1.2484\n",
      "Epoch [10/20], Step [17/2839], Loss: 1.0835\n",
      "Epoch [10/20], Step [18/2839], Loss: 1.2104\n",
      "Epoch [10/20], Step [19/2839], Loss: 0.8371\n",
      "Epoch [10/20], Step [20/2839], Loss: 1.4400\n",
      "Epoch [10/20], Step [21/2839], Loss: 0.7052\n",
      "Epoch [10/20], Step [22/2839], Loss: 1.3776\n",
      "Epoch [10/20], Step [23/2839], Loss: 0.9966\n",
      "Epoch [10/20], Step [24/2839], Loss: 0.8637\n",
      "Epoch [10/20], Step [25/2839], Loss: 0.7291\n",
      "Epoch [10/20], Step [26/2839], Loss: 0.3150\n",
      "Epoch [10/20], Step [27/2839], Loss: 0.5606\n",
      "Epoch [10/20], Step [28/2839], Loss: 0.4282\n",
      "Epoch [10/20], Step [29/2839], Loss: 0.3736\n",
      "Epoch [10/20], Step [30/2839], Loss: 0.5115\n",
      "Epoch [10/20], Step [31/2839], Loss: 0.8159\n",
      "Epoch [10/20], Step [32/2839], Loss: 0.3474\n",
      "Epoch [10/20], Step [33/2839], Loss: 0.3060\n",
      "Epoch [10/20], Step [34/2839], Loss: 0.4261\n",
      "Epoch [10/20], Step [35/2839], Loss: 0.5034\n",
      "Epoch [10/20], Step [36/2839], Loss: 0.6472\n",
      "Epoch [10/20], Step [37/2839], Loss: 0.3504\n",
      "Epoch [10/20], Step [38/2839], Loss: 0.5853\n",
      "Epoch [10/20], Step [39/2839], Loss: 0.8429\n",
      "Epoch [10/20], Step [40/2839], Loss: 0.8450\n",
      "Epoch [10/20], Step [41/2839], Loss: 0.6511\n",
      "Epoch [10/20], Step [42/2839], Loss: 0.3188\n",
      "Epoch [10/20], Step [43/2839], Loss: 1.7913\n",
      "Epoch [10/20], Step [44/2839], Loss: 0.9581\n",
      "Epoch [10/20], Step [45/2839], Loss: 0.7107\n",
      "Epoch [10/20], Step [46/2839], Loss: 0.5080\n",
      "Epoch [10/20], Step [47/2839], Loss: 0.7054\n",
      "Epoch [10/20], Step [48/2839], Loss: 1.0808\n",
      "Epoch [10/20], Step [49/2839], Loss: 0.6423\n",
      "Epoch [10/20], Step [50/2839], Loss: 0.7337\n",
      "Epoch [10/20], Step [51/2839], Loss: 1.1804\n",
      "Epoch [10/20], Step [52/2839], Loss: 0.5025\n",
      "Epoch [10/20], Step [53/2839], Loss: 0.7646\n",
      "Epoch [10/20], Step [54/2839], Loss: 0.8116\n",
      "Epoch [10/20], Step [55/2839], Loss: 0.2474\n",
      "Epoch [10/20], Step [56/2839], Loss: 0.6038\n",
      "Epoch [10/20], Step [57/2839], Loss: 0.4287\n",
      "Epoch [10/20], Step [58/2839], Loss: 0.7798\n",
      "Epoch [10/20], Step [59/2839], Loss: 0.7073\n",
      "Epoch [10/20], Step [60/2839], Loss: 1.3890\n",
      "Epoch [10/20], Step [61/2839], Loss: 0.5850\n",
      "Epoch [10/20], Step [62/2839], Loss: 0.9628\n",
      "Epoch [10/20], Step [63/2839], Loss: 0.5750\n",
      "Epoch [10/20], Step [64/2839], Loss: 0.2677\n",
      "Epoch [10/20], Step [65/2839], Loss: 0.8294\n",
      "Epoch [10/20], Step [66/2839], Loss: 1.0164\n",
      "Epoch [10/20], Step [67/2839], Loss: 0.5118\n",
      "Epoch [10/20], Step [68/2839], Loss: 0.6819\n",
      "Epoch [10/20], Step [69/2839], Loss: 1.0110\n",
      "Epoch [10/20], Step [70/2839], Loss: 0.8766\n",
      "Epoch [10/20], Step [71/2839], Loss: 0.6673\n",
      "Epoch [10/20], Step [72/2839], Loss: 0.2981\n",
      "Epoch [10/20], Step [73/2839], Loss: 0.2429\n",
      "Epoch [10/20], Step [74/2839], Loss: 0.3928\n",
      "Epoch [10/20], Step [75/2839], Loss: 0.5858\n",
      "Epoch [10/20], Step [76/2839], Loss: 0.2081\n",
      "Epoch [10/20], Step [77/2839], Loss: 0.5204\n",
      "Epoch [10/20], Step [78/2839], Loss: 0.8579\n",
      "Epoch [10/20], Step [79/2839], Loss: 0.5498\n",
      "Epoch [10/20], Step [80/2839], Loss: 0.9429\n",
      "Epoch [10/20], Step [81/2839], Loss: 0.6765\n",
      "Epoch [10/20], Step [82/2839], Loss: 0.4282\n",
      "Epoch [10/20], Step [83/2839], Loss: 0.7570\n",
      "Epoch [10/20], Step [84/2839], Loss: 0.6805\n",
      "Epoch [10/20], Step [85/2839], Loss: 0.3476\n",
      "Epoch [10/20], Step [86/2839], Loss: 0.7659\n",
      "Epoch [10/20], Step [87/2839], Loss: 1.0443\n",
      "Epoch [10/20], Step [88/2839], Loss: 0.3897\n",
      "Epoch [10/20], Step [89/2839], Loss: 0.2853\n",
      "Epoch [10/20], Step [90/2839], Loss: 0.4325\n",
      "Epoch [10/20], Step [91/2839], Loss: 1.2859\n",
      "Epoch [10/20], Step [92/2839], Loss: 0.5140\n",
      "Epoch [10/20], Step [93/2839], Loss: 0.6457\n",
      "Epoch [10/20], Step [94/2839], Loss: 0.9938\n",
      "Epoch [10/20], Step [95/2839], Loss: 1.5498\n",
      "Epoch [10/20], Step [96/2839], Loss: 0.8447\n",
      "Epoch [10/20], Step [97/2839], Loss: 0.8648\n",
      "Epoch [10/20], Step [98/2839], Loss: 0.3649\n",
      "Epoch [10/20], Step [99/2839], Loss: 0.5171\n",
      "Epoch [10/20], Step [100/2839], Loss: 0.8702\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[1.043317877179228, 1.0885121928417705, 0.3887199000607209, 0.6506525084022006]\n",
      "Epoch [11/20], Step [1/2839], Loss: 0.9944\n",
      "Epoch [11/20], Step [2/2839], Loss: 1.7355\n",
      "Epoch [11/20], Step [3/2839], Loss: 0.3234\n",
      "Epoch [11/20], Step [4/2839], Loss: 1.2201\n",
      "Epoch [11/20], Step [5/2839], Loss: 0.7142\n",
      "Epoch [11/20], Step [6/2839], Loss: 0.5286\n",
      "Epoch [11/20], Step [7/2839], Loss: 0.7851\n",
      "Epoch [11/20], Step [8/2839], Loss: 1.7653\n",
      "Epoch [11/20], Step [9/2839], Loss: 0.9956\n",
      "Epoch [11/20], Step [10/2839], Loss: 1.0995\n",
      "Epoch [11/20], Step [11/2839], Loss: 1.8599\n",
      "Epoch [11/20], Step [12/2839], Loss: 1.8072\n",
      "Epoch [11/20], Step [13/2839], Loss: 1.3288\n",
      "Epoch [11/20], Step [14/2839], Loss: 2.0625\n",
      "Epoch [11/20], Step [15/2839], Loss: 0.7579\n",
      "Epoch [11/20], Step [16/2839], Loss: 1.6960\n",
      "Epoch [11/20], Step [17/2839], Loss: 1.6177\n",
      "Epoch [11/20], Step [18/2839], Loss: 1.1327\n",
      "Epoch [11/20], Step [19/2839], Loss: 0.8808\n",
      "Epoch [11/20], Step [20/2839], Loss: 1.5464\n",
      "Epoch [11/20], Step [21/2839], Loss: 1.0577\n",
      "Epoch [11/20], Step [22/2839], Loss: 0.3184\n",
      "Epoch [11/20], Step [23/2839], Loss: 0.6747\n",
      "Epoch [11/20], Step [24/2839], Loss: 1.2023\n",
      "Epoch [11/20], Step [25/2839], Loss: 0.4866\n",
      "Epoch [11/20], Step [26/2839], Loss: 0.6424\n",
      "Epoch [11/20], Step [27/2839], Loss: 1.4080\n",
      "Epoch [11/20], Step [28/2839], Loss: 0.7756\n",
      "Epoch [11/20], Step [29/2839], Loss: 0.3377\n",
      "Epoch [11/20], Step [30/2839], Loss: 0.5483\n",
      "Epoch [11/20], Step [31/2839], Loss: 1.4090\n",
      "Epoch [11/20], Step [32/2839], Loss: 0.6028\n",
      "Epoch [11/20], Step [33/2839], Loss: 0.7041\n",
      "Epoch [11/20], Step [34/2839], Loss: 0.6647\n",
      "Epoch [11/20], Step [35/2839], Loss: 0.5912\n",
      "Epoch [11/20], Step [36/2839], Loss: 1.7493\n",
      "Epoch [11/20], Step [37/2839], Loss: 0.6249\n",
      "Epoch [11/20], Step [38/2839], Loss: 0.8250\n",
      "Epoch [11/20], Step [39/2839], Loss: 0.4629\n",
      "Epoch [11/20], Step [40/2839], Loss: 1.2159\n",
      "Epoch [11/20], Step [41/2839], Loss: 0.8350\n",
      "Epoch [11/20], Step [42/2839], Loss: 0.6918\n",
      "Epoch [11/20], Step [43/2839], Loss: 0.9497\n",
      "Epoch [11/20], Step [44/2839], Loss: 0.9168\n",
      "Epoch [11/20], Step [45/2839], Loss: 0.7969\n",
      "Epoch [11/20], Step [46/2839], Loss: 0.9592\n",
      "Epoch [11/20], Step [47/2839], Loss: 1.4202\n",
      "Epoch [11/20], Step [48/2839], Loss: 0.4552\n",
      "Epoch [11/20], Step [49/2839], Loss: 0.6011\n",
      "Epoch [11/20], Step [50/2839], Loss: 0.4226\n",
      "Epoch [11/20], Step [51/2839], Loss: 0.5464\n",
      "Epoch [11/20], Step [52/2839], Loss: 0.5625\n",
      "Epoch [11/20], Step [53/2839], Loss: 0.5562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Step [54/2839], Loss: 0.4593\n",
      "Epoch [11/20], Step [55/2839], Loss: 0.9285\n",
      "Epoch [11/20], Step [56/2839], Loss: 0.6228\n",
      "Epoch [11/20], Step [57/2839], Loss: 0.4587\n",
      "Epoch [11/20], Step [58/2839], Loss: 1.2724\n",
      "Epoch [11/20], Step [59/2839], Loss: 0.7749\n",
      "Epoch [11/20], Step [60/2839], Loss: 0.8056\n",
      "Epoch [11/20], Step [61/2839], Loss: 1.1432\n",
      "Epoch [11/20], Step [62/2839], Loss: 1.3491\n",
      "Epoch [11/20], Step [63/2839], Loss: 1.4813\n",
      "Epoch [11/20], Step [64/2839], Loss: 0.9358\n",
      "Epoch [11/20], Step [65/2839], Loss: 1.5011\n",
      "Epoch [11/20], Step [66/2839], Loss: 0.7419\n",
      "Epoch [11/20], Step [67/2839], Loss: 1.4602\n",
      "Epoch [11/20], Step [68/2839], Loss: 0.8346\n",
      "Epoch [11/20], Step [69/2839], Loss: 0.4111\n",
      "Epoch [11/20], Step [70/2839], Loss: 0.6335\n",
      "Epoch [11/20], Step [71/2839], Loss: 1.5743\n",
      "Epoch [11/20], Step [72/2839], Loss: 0.5524\n",
      "Epoch [11/20], Step [73/2839], Loss: 0.9104\n",
      "Epoch [11/20], Step [74/2839], Loss: 0.8636\n",
      "Epoch [11/20], Step [75/2839], Loss: 0.5918\n",
      "Epoch [11/20], Step [76/2839], Loss: 0.9026\n",
      "Epoch [11/20], Step [77/2839], Loss: 2.3207\n",
      "Epoch [11/20], Step [78/2839], Loss: 0.9992\n",
      "Epoch [11/20], Step [79/2839], Loss: 1.0623\n",
      "Epoch [11/20], Step [80/2839], Loss: 1.1180\n",
      "Epoch [11/20], Step [81/2839], Loss: 1.0485\n",
      "Epoch [11/20], Step [82/2839], Loss: 0.3927\n",
      "Epoch [11/20], Step [83/2839], Loss: 1.1438\n",
      "Epoch [11/20], Step [84/2839], Loss: 0.2999\n",
      "Epoch [11/20], Step [85/2839], Loss: 0.4715\n",
      "Epoch [11/20], Step [86/2839], Loss: 0.8116\n",
      "Epoch [11/20], Step [87/2839], Loss: 0.8881\n",
      "Epoch [11/20], Step [88/2839], Loss: 0.5690\n",
      "Epoch [11/20], Step [89/2839], Loss: 0.5335\n",
      "Epoch [11/20], Step [90/2839], Loss: 0.1519\n",
      "Epoch [11/20], Step [91/2839], Loss: 0.6210\n",
      "Epoch [11/20], Step [92/2839], Loss: 1.7839\n",
      "Epoch [11/20], Step [93/2839], Loss: 0.8869\n",
      "Epoch [11/20], Step [94/2839], Loss: 1.1693\n",
      "Epoch [11/20], Step [95/2839], Loss: 0.7482\n",
      "Epoch [11/20], Step [96/2839], Loss: 0.5736\n",
      "Epoch [11/20], Step [97/2839], Loss: 0.5283\n",
      "Epoch [11/20], Step [98/2839], Loss: 1.0852\n",
      "Epoch [11/20], Step [99/2839], Loss: 0.3526\n",
      "Epoch [11/20], Step [100/2839], Loss: 1.0596\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[0.8117970560007652, 0.6590144601315095, 0.4132378542655408, 0.6639751346283621]\n",
      "Epoch [12/20], Step [1/2839], Loss: 0.6891\n",
      "Epoch [12/20], Step [2/2839], Loss: 0.3327\n",
      "Epoch [12/20], Step [3/2839], Loss: 0.6228\n",
      "Epoch [12/20], Step [4/2839], Loss: 0.6777\n",
      "Epoch [12/20], Step [5/2839], Loss: 0.7276\n",
      "Epoch [12/20], Step [6/2839], Loss: 0.7829\n",
      "Epoch [12/20], Step [7/2839], Loss: 0.8214\n",
      "Epoch [12/20], Step [8/2839], Loss: 0.3604\n",
      "Epoch [12/20], Step [9/2839], Loss: 1.8807\n",
      "Epoch [12/20], Step [10/2839], Loss: 0.5613\n",
      "Epoch [12/20], Step [11/2839], Loss: 1.2916\n",
      "Epoch [12/20], Step [12/2839], Loss: 0.8159\n",
      "Epoch [12/20], Step [13/2839], Loss: 1.3787\n",
      "Epoch [12/20], Step [14/2839], Loss: 0.3659\n",
      "Epoch [12/20], Step [15/2839], Loss: 1.3381\n",
      "Epoch [12/20], Step [16/2839], Loss: 0.9727\n",
      "Epoch [12/20], Step [17/2839], Loss: 0.4807\n",
      "Epoch [12/20], Step [18/2839], Loss: 0.5964\n",
      "Epoch [12/20], Step [19/2839], Loss: 1.0505\n",
      "Epoch [12/20], Step [20/2839], Loss: 0.5777\n",
      "Epoch [12/20], Step [21/2839], Loss: 0.7028\n",
      "Epoch [12/20], Step [22/2839], Loss: 0.6725\n",
      "Epoch [12/20], Step [23/2839], Loss: 0.4385\n",
      "Epoch [12/20], Step [24/2839], Loss: 0.2666\n",
      "Epoch [12/20], Step [25/2839], Loss: 0.8776\n",
      "Epoch [12/20], Step [26/2839], Loss: 0.4803\n",
      "Epoch [12/20], Step [27/2839], Loss: 0.4913\n",
      "Epoch [12/20], Step [28/2839], Loss: 0.2914\n",
      "Epoch [12/20], Step [29/2839], Loss: 0.4811\n",
      "Epoch [12/20], Step [30/2839], Loss: 0.6652\n",
      "Epoch [12/20], Step [31/2839], Loss: 0.7595\n",
      "Epoch [12/20], Step [32/2839], Loss: 0.7298\n",
      "Epoch [12/20], Step [33/2839], Loss: 1.0647\n",
      "Epoch [12/20], Step [34/2839], Loss: 0.4949\n",
      "Epoch [12/20], Step [35/2839], Loss: 0.3932\n",
      "Epoch [12/20], Step [36/2839], Loss: 0.6400\n",
      "Epoch [12/20], Step [37/2839], Loss: 1.4625\n",
      "Epoch [12/20], Step [38/2839], Loss: 0.4505\n",
      "Epoch [12/20], Step [39/2839], Loss: 1.3069\n",
      "Epoch [12/20], Step [40/2839], Loss: 0.5979\n",
      "Epoch [12/20], Step [41/2839], Loss: 0.3493\n",
      "Epoch [12/20], Step [42/2839], Loss: 0.7945\n",
      "Epoch [12/20], Step [43/2839], Loss: 0.1592\n",
      "Epoch [12/20], Step [44/2839], Loss: 0.3026\n",
      "Epoch [12/20], Step [45/2839], Loss: 1.0353\n",
      "Epoch [12/20], Step [46/2839], Loss: 0.3180\n",
      "Epoch [12/20], Step [47/2839], Loss: 0.9952\n",
      "Epoch [12/20], Step [48/2839], Loss: 0.7518\n",
      "Epoch [12/20], Step [49/2839], Loss: 0.5125\n",
      "Epoch [12/20], Step [50/2839], Loss: 0.6185\n",
      "Epoch [12/20], Step [51/2839], Loss: 0.4620\n",
      "Epoch [12/20], Step [52/2839], Loss: 0.3007\n",
      "Epoch [12/20], Step [53/2839], Loss: 0.4035\n",
      "Epoch [12/20], Step [54/2839], Loss: 0.2097\n",
      "Epoch [12/20], Step [55/2839], Loss: 0.6032\n",
      "Epoch [12/20], Step [56/2839], Loss: 0.3725\n",
      "Epoch [12/20], Step [57/2839], Loss: 0.2755\n",
      "Epoch [12/20], Step [58/2839], Loss: 0.6402\n",
      "Epoch [12/20], Step [59/2839], Loss: 0.4669\n",
      "Epoch [12/20], Step [60/2839], Loss: 0.5896\n",
      "Epoch [12/20], Step [61/2839], Loss: 0.5273\n",
      "Epoch [12/20], Step [62/2839], Loss: 0.3659\n",
      "Epoch [12/20], Step [63/2839], Loss: 0.3730\n",
      "Epoch [12/20], Step [64/2839], Loss: 0.3780\n",
      "Epoch [12/20], Step [65/2839], Loss: 0.5835\n",
      "Epoch [12/20], Step [66/2839], Loss: 1.2076\n",
      "Epoch [12/20], Step [67/2839], Loss: 0.5261\n",
      "Epoch [12/20], Step [68/2839], Loss: 0.5756\n",
      "Epoch [12/20], Step [69/2839], Loss: 0.5505\n",
      "Epoch [12/20], Step [70/2839], Loss: 1.0527\n",
      "Epoch [12/20], Step [71/2839], Loss: 1.1532\n",
      "Epoch [12/20], Step [72/2839], Loss: 0.1956\n",
      "Epoch [12/20], Step [73/2839], Loss: 0.4092\n",
      "Epoch [12/20], Step [74/2839], Loss: 1.0066\n",
      "Epoch [12/20], Step [75/2839], Loss: 0.8885\n",
      "Epoch [12/20], Step [76/2839], Loss: 0.7847\n",
      "Epoch [12/20], Step [77/2839], Loss: 0.7017\n",
      "Epoch [12/20], Step [78/2839], Loss: 0.6150\n",
      "Epoch [12/20], Step [79/2839], Loss: 0.5770\n",
      "Epoch [12/20], Step [80/2839], Loss: 0.6699\n",
      "Epoch [12/20], Step [81/2839], Loss: 0.6840\n",
      "Epoch [12/20], Step [82/2839], Loss: 1.1392\n",
      "Epoch [12/20], Step [83/2839], Loss: 0.1736\n",
      "Epoch [12/20], Step [84/2839], Loss: 0.8555\n",
      "Epoch [12/20], Step [85/2839], Loss: 0.4089\n",
      "Epoch [12/20], Step [86/2839], Loss: 0.1301\n",
      "Epoch [12/20], Step [87/2839], Loss: 0.9121\n",
      "Epoch [12/20], Step [88/2839], Loss: 1.0090\n",
      "Epoch [12/20], Step [89/2839], Loss: 0.5573\n",
      "Epoch [12/20], Step [90/2839], Loss: 1.2048\n",
      "Epoch [12/20], Step [91/2839], Loss: 0.7712\n",
      "Epoch [12/20], Step [92/2839], Loss: 0.7242\n",
      "Epoch [12/20], Step [93/2839], Loss: 0.6781\n",
      "Epoch [12/20], Step [94/2839], Loss: 0.6215\n",
      "Epoch [12/20], Step [95/2839], Loss: 0.5087\n",
      "Epoch [12/20], Step [96/2839], Loss: 0.4290\n",
      "Epoch [12/20], Step [97/2839], Loss: 0.6052\n",
      "Epoch [12/20], Step [98/2839], Loss: 0.6340\n",
      "Epoch [12/20], Step [99/2839], Loss: 0.2004\n",
      "Epoch [12/20], Step [100/2839], Loss: 1.3314\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[0.9022459009768193, 0.8140476658294725, 0.4452439350257932, 0.6709624286526702]\n",
      "Epoch [13/20], Step [1/2839], Loss: 1.0527\n",
      "Epoch [13/20], Step [2/2839], Loss: 0.2463\n",
      "Epoch [13/20], Step [3/2839], Loss: 0.4782\n",
      "Epoch [13/20], Step [4/2839], Loss: 0.7729\n",
      "Epoch [13/20], Step [5/2839], Loss: 0.2721\n",
      "Epoch [13/20], Step [6/2839], Loss: 0.3458\n",
      "Epoch [13/20], Step [7/2839], Loss: 0.3346\n",
      "Epoch [13/20], Step [8/2839], Loss: 0.6468\n",
      "Epoch [13/20], Step [9/2839], Loss: 1.1275\n",
      "Epoch [13/20], Step [10/2839], Loss: 0.6363\n",
      "Epoch [13/20], Step [11/2839], Loss: 1.1093\n",
      "Epoch [13/20], Step [12/2839], Loss: 0.4665\n",
      "Epoch [13/20], Step [13/2839], Loss: 0.6287\n",
      "Epoch [13/20], Step [14/2839], Loss: 0.3674\n",
      "Epoch [13/20], Step [15/2839], Loss: 0.6792\n",
      "Epoch [13/20], Step [16/2839], Loss: 0.3057\n",
      "Epoch [13/20], Step [17/2839], Loss: 0.6300\n",
      "Epoch [13/20], Step [18/2839], Loss: 0.7920\n",
      "Epoch [13/20], Step [19/2839], Loss: 0.3688\n",
      "Epoch [13/20], Step [20/2839], Loss: 0.3463\n",
      "Epoch [13/20], Step [21/2839], Loss: 0.2809\n",
      "Epoch [13/20], Step [22/2839], Loss: 0.9442\n",
      "Epoch [13/20], Step [23/2839], Loss: 0.3067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Step [24/2839], Loss: 0.3890\n",
      "Epoch [13/20], Step [25/2839], Loss: 0.7594\n",
      "Epoch [13/20], Step [26/2839], Loss: 0.4418\n",
      "Epoch [13/20], Step [27/2839], Loss: 0.7807\n",
      "Epoch [13/20], Step [28/2839], Loss: 0.8639\n",
      "Epoch [13/20], Step [29/2839], Loss: 0.9366\n",
      "Epoch [13/20], Step [30/2839], Loss: 0.6926\n",
      "Epoch [13/20], Step [31/2839], Loss: 0.4339\n",
      "Epoch [13/20], Step [32/2839], Loss: 0.4861\n",
      "Epoch [13/20], Step [33/2839], Loss: 0.7695\n",
      "Epoch [13/20], Step [34/2839], Loss: 1.0630\n",
      "Epoch [13/20], Step [35/2839], Loss: 0.4249\n",
      "Epoch [13/20], Step [36/2839], Loss: 0.5573\n",
      "Epoch [13/20], Step [37/2839], Loss: 2.3080\n",
      "Epoch [13/20], Step [38/2839], Loss: 0.4349\n",
      "Epoch [13/20], Step [39/2839], Loss: 1.0362\n",
      "Epoch [13/20], Step [40/2839], Loss: 0.9050\n",
      "Epoch [13/20], Step [41/2839], Loss: 0.3776\n",
      "Epoch [13/20], Step [42/2839], Loss: 0.5371\n",
      "Epoch [13/20], Step [43/2839], Loss: 0.3982\n",
      "Epoch [13/20], Step [44/2839], Loss: 0.8552\n",
      "Epoch [13/20], Step [45/2839], Loss: 0.2841\n",
      "Epoch [13/20], Step [46/2839], Loss: 1.0799\n",
      "Epoch [13/20], Step [47/2839], Loss: 0.6126\n",
      "Epoch [13/20], Step [48/2839], Loss: 0.6197\n",
      "Epoch [13/20], Step [49/2839], Loss: 0.9266\n",
      "Epoch [13/20], Step [50/2839], Loss: 0.5666\n",
      "Epoch [13/20], Step [51/2839], Loss: 0.5064\n",
      "Epoch [13/20], Step [52/2839], Loss: 0.5556\n",
      "Epoch [13/20], Step [53/2839], Loss: 0.9316\n",
      "Epoch [13/20], Step [54/2839], Loss: 0.7806\n",
      "Epoch [13/20], Step [55/2839], Loss: 0.5027\n",
      "Epoch [13/20], Step [56/2839], Loss: 0.6000\n",
      "Epoch [13/20], Step [57/2839], Loss: 0.9106\n",
      "Epoch [13/20], Step [58/2839], Loss: 0.7739\n",
      "Epoch [13/20], Step [59/2839], Loss: 0.6769\n",
      "Epoch [13/20], Step [60/2839], Loss: 0.7663\n",
      "Epoch [13/20], Step [61/2839], Loss: 0.7147\n",
      "Epoch [13/20], Step [62/2839], Loss: 0.5643\n",
      "Epoch [13/20], Step [63/2839], Loss: 0.2083\n",
      "Epoch [13/20], Step [64/2839], Loss: 0.3796\n",
      "Epoch [13/20], Step [65/2839], Loss: 0.8495\n",
      "Epoch [13/20], Step [66/2839], Loss: 0.8143\n",
      "Epoch [13/20], Step [67/2839], Loss: 0.6091\n",
      "Epoch [13/20], Step [68/2839], Loss: 0.3724\n",
      "Epoch [13/20], Step [69/2839], Loss: 1.0165\n",
      "Epoch [13/20], Step [70/2839], Loss: 0.5358\n",
      "Epoch [13/20], Step [71/2839], Loss: 1.0223\n",
      "Epoch [13/20], Step [72/2839], Loss: 0.7558\n",
      "Epoch [13/20], Step [73/2839], Loss: 0.6825\n",
      "Epoch [13/20], Step [74/2839], Loss: 0.6785\n",
      "Epoch [13/20], Step [75/2839], Loss: 1.2742\n",
      "Epoch [13/20], Step [76/2839], Loss: 0.1359\n",
      "Epoch [13/20], Step [77/2839], Loss: 0.8414\n",
      "Epoch [13/20], Step [78/2839], Loss: 0.5474\n",
      "Epoch [13/20], Step [79/2839], Loss: 0.5236\n",
      "Epoch [13/20], Step [80/2839], Loss: 0.4757\n",
      "Epoch [13/20], Step [81/2839], Loss: 0.2808\n",
      "Epoch [13/20], Step [82/2839], Loss: 0.3646\n",
      "Epoch [13/20], Step [83/2839], Loss: 0.6589\n",
      "Epoch [13/20], Step [84/2839], Loss: 0.6531\n",
      "Epoch [13/20], Step [85/2839], Loss: 0.6190\n",
      "Epoch [13/20], Step [86/2839], Loss: 0.4211\n",
      "Epoch [13/20], Step [87/2839], Loss: 0.5595\n",
      "Epoch [13/20], Step [88/2839], Loss: 0.6914\n",
      "Epoch [13/20], Step [89/2839], Loss: 0.6959\n",
      "Epoch [13/20], Step [90/2839], Loss: 0.5781\n",
      "Epoch [13/20], Step [91/2839], Loss: 1.0782\n",
      "Epoch [13/20], Step [92/2839], Loss: 0.6231\n",
      "Epoch [13/20], Step [93/2839], Loss: 0.7135\n",
      "Epoch [13/20], Step [94/2839], Loss: 0.4927\n",
      "Epoch [13/20], Step [95/2839], Loss: 0.4289\n",
      "Epoch [13/20], Step [96/2839], Loss: 1.0044\n",
      "Epoch [13/20], Step [97/2839], Loss: 0.5517\n",
      "Epoch [13/20], Step [98/2839], Loss: 0.5581\n",
      "Epoch [13/20], Step [99/2839], Loss: 0.8434\n",
      "Epoch [13/20], Step [100/2839], Loss: 0.6271\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[0.773452812240838, 0.5982292527632609, 0.4889733428798441, 0.6826154869212866]\n",
      "Epoch [14/20], Step [1/2839], Loss: 1.0926\n",
      "Epoch [14/20], Step [2/2839], Loss: 1.3003\n",
      "Epoch [14/20], Step [3/2839], Loss: 1.3955\n",
      "Epoch [14/20], Step [4/2839], Loss: 0.6984\n",
      "Epoch [14/20], Step [5/2839], Loss: 0.4451\n",
      "Epoch [14/20], Step [6/2839], Loss: 0.6347\n",
      "Epoch [14/20], Step [7/2839], Loss: 0.7582\n",
      "Epoch [14/20], Step [8/2839], Loss: 0.2531\n",
      "Epoch [14/20], Step [9/2839], Loss: 0.3702\n",
      "Epoch [14/20], Step [10/2839], Loss: 0.4283\n",
      "Epoch [14/20], Step [11/2839], Loss: 0.6693\n",
      "Epoch [14/20], Step [12/2839], Loss: 0.6927\n",
      "Epoch [14/20], Step [13/2839], Loss: 0.5049\n",
      "Epoch [14/20], Step [14/2839], Loss: 0.8672\n",
      "Epoch [14/20], Step [15/2839], Loss: 0.3065\n",
      "Epoch [14/20], Step [16/2839], Loss: 0.7071\n",
      "Epoch [14/20], Step [17/2839], Loss: 0.7810\n",
      "Epoch [14/20], Step [18/2839], Loss: 0.9006\n",
      "Epoch [14/20], Step [19/2839], Loss: 0.4120\n",
      "Epoch [14/20], Step [20/2839], Loss: 0.4511\n",
      "Epoch [14/20], Step [21/2839], Loss: 0.5796\n",
      "Epoch [14/20], Step [22/2839], Loss: 0.2242\n",
      "Epoch [14/20], Step [23/2839], Loss: 0.6943\n",
      "Epoch [14/20], Step [24/2839], Loss: 0.6301\n",
      "Epoch [14/20], Step [25/2839], Loss: 0.6524\n",
      "Epoch [14/20], Step [26/2839], Loss: 0.4388\n",
      "Epoch [14/20], Step [27/2839], Loss: 0.9826\n",
      "Epoch [14/20], Step [28/2839], Loss: 0.9388\n",
      "Epoch [14/20], Step [29/2839], Loss: 0.4487\n",
      "Epoch [14/20], Step [30/2839], Loss: 0.7348\n",
      "Epoch [14/20], Step [31/2839], Loss: 0.5623\n",
      "Epoch [14/20], Step [32/2839], Loss: 1.0309\n",
      "Epoch [14/20], Step [33/2839], Loss: 0.4092\n",
      "Epoch [14/20], Step [34/2839], Loss: 0.7901\n",
      "Epoch [14/20], Step [35/2839], Loss: 0.4152\n",
      "Epoch [14/20], Step [36/2839], Loss: 0.4647\n",
      "Epoch [14/20], Step [37/2839], Loss: 0.6668\n",
      "Epoch [14/20], Step [38/2839], Loss: 0.5135\n",
      "Epoch [14/20], Step [39/2839], Loss: 0.4797\n",
      "Epoch [14/20], Step [40/2839], Loss: 0.4672\n",
      "Epoch [14/20], Step [41/2839], Loss: 0.6126\n",
      "Epoch [14/20], Step [42/2839], Loss: 0.2750\n",
      "Epoch [14/20], Step [43/2839], Loss: 0.8259\n",
      "Epoch [14/20], Step [44/2839], Loss: 0.6685\n",
      "Epoch [14/20], Step [45/2839], Loss: 0.8394\n",
      "Epoch [14/20], Step [46/2839], Loss: 0.2759\n",
      "Epoch [14/20], Step [47/2839], Loss: 0.8642\n",
      "Epoch [14/20], Step [48/2839], Loss: 0.4982\n",
      "Epoch [14/20], Step [49/2839], Loss: 0.4587\n",
      "Epoch [14/20], Step [50/2839], Loss: 0.1327\n",
      "Epoch [14/20], Step [51/2839], Loss: 0.6713\n",
      "Epoch [14/20], Step [52/2839], Loss: 0.2641\n",
      "Epoch [14/20], Step [53/2839], Loss: 0.3857\n",
      "Epoch [14/20], Step [54/2839], Loss: 0.5261\n",
      "Epoch [14/20], Step [55/2839], Loss: 0.5128\n",
      "Epoch [14/20], Step [56/2839], Loss: 0.4602\n",
      "Epoch [14/20], Step [57/2839], Loss: 0.5630\n",
      "Epoch [14/20], Step [58/2839], Loss: 0.9157\n",
      "Epoch [14/20], Step [59/2839], Loss: 0.5900\n",
      "Epoch [14/20], Step [60/2839], Loss: 0.4606\n",
      "Epoch [14/20], Step [61/2839], Loss: 0.4746\n",
      "Epoch [14/20], Step [62/2839], Loss: 0.4679\n",
      "Epoch [14/20], Step [63/2839], Loss: 0.8233\n",
      "Epoch [14/20], Step [64/2839], Loss: 1.1318\n",
      "Epoch [14/20], Step [65/2839], Loss: 0.9946\n",
      "Epoch [14/20], Step [66/2839], Loss: 0.9908\n",
      "Epoch [14/20], Step [67/2839], Loss: 0.2746\n",
      "Epoch [14/20], Step [68/2839], Loss: 1.0837\n",
      "Epoch [14/20], Step [69/2839], Loss: 0.6374\n",
      "Epoch [14/20], Step [70/2839], Loss: 0.7730\n",
      "Epoch [14/20], Step [71/2839], Loss: 0.4823\n",
      "Epoch [14/20], Step [72/2839], Loss: 0.7211\n",
      "Epoch [14/20], Step [73/2839], Loss: 0.5513\n",
      "Epoch [14/20], Step [74/2839], Loss: 0.6322\n",
      "Epoch [14/20], Step [75/2839], Loss: 0.7761\n",
      "Epoch [14/20], Step [76/2839], Loss: 0.8134\n",
      "Epoch [14/20], Step [77/2839], Loss: 0.1697\n",
      "Epoch [14/20], Step [78/2839], Loss: 1.2851\n",
      "Epoch [14/20], Step [79/2839], Loss: 0.9799\n",
      "Epoch [14/20], Step [80/2839], Loss: 0.6066\n",
      "Epoch [14/20], Step [81/2839], Loss: 1.0432\n",
      "Epoch [14/20], Step [82/2839], Loss: 1.9051\n",
      "Epoch [14/20], Step [83/2839], Loss: 0.5856\n",
      "Epoch [14/20], Step [84/2839], Loss: 0.8748\n",
      "Epoch [14/20], Step [85/2839], Loss: 0.7833\n",
      "Epoch [14/20], Step [86/2839], Loss: 1.0751\n",
      "Epoch [14/20], Step [87/2839], Loss: 0.6566\n",
      "Epoch [14/20], Step [88/2839], Loss: 0.9115\n",
      "Epoch [14/20], Step [89/2839], Loss: 0.6375\n",
      "Epoch [14/20], Step [90/2839], Loss: 0.4954\n",
      "Epoch [14/20], Step [91/2839], Loss: 1.8507\n",
      "Epoch [14/20], Step [92/2839], Loss: 0.4914\n",
      "Epoch [14/20], Step [93/2839], Loss: 0.2957\n",
      "Epoch [14/20], Step [94/2839], Loss: 0.1869\n",
      "Epoch [14/20], Step [95/2839], Loss: 0.4194\n",
      "Epoch [14/20], Step [96/2839], Loss: 0.8241\n",
      "Epoch [14/20], Step [97/2839], Loss: 0.4686\n",
      "Epoch [14/20], Step [98/2839], Loss: 1.1641\n",
      "Epoch [14/20], Step [99/2839], Loss: 0.1595\n",
      "Epoch [14/20], Step [100/2839], Loss: 0.2561\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[0.7391941334353079, 0.5464079669051758, 0.48633565472202694, 0.6832203484092103]\n",
      "rmse improved at epoch  14 ; best_mse,best_ci,best_r: 0.5464079669051758 0.6832203484092103 0.48633565472202694\n",
      "Epoch [15/20], Step [1/2839], Loss: 0.8161\n",
      "Epoch [15/20], Step [2/2839], Loss: 0.5105\n",
      "Epoch [15/20], Step [3/2839], Loss: 0.9766\n",
      "Epoch [15/20], Step [4/2839], Loss: 0.6003\n",
      "Epoch [15/20], Step [5/2839], Loss: 0.5610\n",
      "Epoch [15/20], Step [6/2839], Loss: 0.4525\n",
      "Epoch [15/20], Step [7/2839], Loss: 0.5167\n",
      "Epoch [15/20], Step [8/2839], Loss: 0.7288\n",
      "Epoch [15/20], Step [9/2839], Loss: 0.5648\n",
      "Epoch [15/20], Step [10/2839], Loss: 0.6747\n",
      "Epoch [15/20], Step [11/2839], Loss: 0.4072\n",
      "Epoch [15/20], Step [12/2839], Loss: 0.3258\n",
      "Epoch [15/20], Step [13/2839], Loss: 0.4424\n",
      "Epoch [15/20], Step [14/2839], Loss: 1.0187\n",
      "Epoch [15/20], Step [15/2839], Loss: 0.3008\n",
      "Epoch [15/20], Step [16/2839], Loss: 0.7580\n",
      "Epoch [15/20], Step [17/2839], Loss: 0.5462\n",
      "Epoch [15/20], Step [18/2839], Loss: 0.6015\n",
      "Epoch [15/20], Step [19/2839], Loss: 0.6387\n",
      "Epoch [15/20], Step [20/2839], Loss: 0.9384\n",
      "Epoch [15/20], Step [21/2839], Loss: 0.4049\n",
      "Epoch [15/20], Step [22/2839], Loss: 0.4657\n",
      "Epoch [15/20], Step [23/2839], Loss: 0.6012\n",
      "Epoch [15/20], Step [24/2839], Loss: 0.4125\n",
      "Epoch [15/20], Step [25/2839], Loss: 0.4633\n",
      "Epoch [15/20], Step [26/2839], Loss: 0.5639\n",
      "Epoch [15/20], Step [27/2839], Loss: 1.0696\n",
      "Epoch [15/20], Step [28/2839], Loss: 0.6971\n",
      "Epoch [15/20], Step [29/2839], Loss: 0.8482\n",
      "Epoch [15/20], Step [30/2839], Loss: 0.3896\n",
      "Epoch [15/20], Step [31/2839], Loss: 0.3728\n",
      "Epoch [15/20], Step [32/2839], Loss: 0.7941\n",
      "Epoch [15/20], Step [33/2839], Loss: 1.0159\n",
      "Epoch [15/20], Step [34/2839], Loss: 0.7665\n",
      "Epoch [15/20], Step [35/2839], Loss: 0.8900\n",
      "Epoch [15/20], Step [36/2839], Loss: 0.7530\n",
      "Epoch [15/20], Step [37/2839], Loss: 0.8216\n",
      "Epoch [15/20], Step [38/2839], Loss: 0.4333\n",
      "Epoch [15/20], Step [39/2839], Loss: 1.6089\n",
      "Epoch [15/20], Step [40/2839], Loss: 0.3372\n",
      "Epoch [15/20], Step [41/2839], Loss: 0.6874\n",
      "Epoch [15/20], Step [42/2839], Loss: 0.4182\n",
      "Epoch [15/20], Step [43/2839], Loss: 0.4380\n",
      "Epoch [15/20], Step [44/2839], Loss: 0.7359\n",
      "Epoch [15/20], Step [45/2839], Loss: 1.6305\n",
      "Epoch [15/20], Step [46/2839], Loss: 0.5030\n",
      "Epoch [15/20], Step [47/2839], Loss: 0.7595\n",
      "Epoch [15/20], Step [48/2839], Loss: 1.0417\n",
      "Epoch [15/20], Step [49/2839], Loss: 0.8671\n",
      "Epoch [15/20], Step [50/2839], Loss: 0.3112\n",
      "Epoch [15/20], Step [51/2839], Loss: 0.4228\n",
      "Epoch [15/20], Step [52/2839], Loss: 0.9623\n",
      "Epoch [15/20], Step [53/2839], Loss: 0.5488\n",
      "Epoch [15/20], Step [54/2839], Loss: 0.5896\n",
      "Epoch [15/20], Step [55/2839], Loss: 0.4174\n",
      "Epoch [15/20], Step [56/2839], Loss: 0.5616\n",
      "Epoch [15/20], Step [57/2839], Loss: 0.7608\n",
      "Epoch [15/20], Step [58/2839], Loss: 0.3780\n",
      "Epoch [15/20], Step [59/2839], Loss: 0.7045\n",
      "Epoch [15/20], Step [60/2839], Loss: 0.7075\n",
      "Epoch [15/20], Step [61/2839], Loss: 0.3602\n",
      "Epoch [15/20], Step [62/2839], Loss: 0.6005\n",
      "Epoch [15/20], Step [63/2839], Loss: 1.0472\n",
      "Epoch [15/20], Step [64/2839], Loss: 0.5151\n",
      "Epoch [15/20], Step [65/2839], Loss: 0.6051\n",
      "Epoch [15/20], Step [66/2839], Loss: 0.5027\n",
      "Epoch [15/20], Step [67/2839], Loss: 0.5794\n",
      "Epoch [15/20], Step [68/2839], Loss: 0.7281\n",
      "Epoch [15/20], Step [69/2839], Loss: 0.5836\n",
      "Epoch [15/20], Step [70/2839], Loss: 0.3839\n",
      "Epoch [15/20], Step [71/2839], Loss: 1.1115\n",
      "Epoch [15/20], Step [72/2839], Loss: 0.5702\n",
      "Epoch [15/20], Step [73/2839], Loss: 0.5695\n",
      "Epoch [15/20], Step [74/2839], Loss: 0.3125\n",
      "Epoch [15/20], Step [75/2839], Loss: 0.7334\n",
      "Epoch [15/20], Step [76/2839], Loss: 0.8584\n",
      "Epoch [15/20], Step [77/2839], Loss: 0.4657\n",
      "Epoch [15/20], Step [78/2839], Loss: 0.5241\n",
      "Epoch [15/20], Step [79/2839], Loss: 0.7337\n",
      "Epoch [15/20], Step [80/2839], Loss: 0.4462\n",
      "Epoch [15/20], Step [81/2839], Loss: 0.3132\n",
      "Epoch [15/20], Step [82/2839], Loss: 0.5572\n",
      "Epoch [15/20], Step [83/2839], Loss: 0.6940\n",
      "Epoch [15/20], Step [84/2839], Loss: 1.3349\n",
      "Epoch [15/20], Step [85/2839], Loss: 0.9722\n",
      "Epoch [15/20], Step [86/2839], Loss: 0.8559\n",
      "Epoch [15/20], Step [87/2839], Loss: 0.7001\n",
      "Epoch [15/20], Step [88/2839], Loss: 0.8379\n",
      "Epoch [15/20], Step [89/2839], Loss: 0.7896\n",
      "Epoch [15/20], Step [90/2839], Loss: 1.2539\n",
      "Epoch [15/20], Step [91/2839], Loss: 0.5504\n",
      "Epoch [15/20], Step [92/2839], Loss: 0.3593\n",
      "Epoch [15/20], Step [93/2839], Loss: 1.2492\n",
      "Epoch [15/20], Step [94/2839], Loss: 0.9660\n",
      "Epoch [15/20], Step [95/2839], Loss: 1.1188\n",
      "Epoch [15/20], Step [96/2839], Loss: 0.8976\n",
      "Epoch [15/20], Step [97/2839], Loss: 0.4550\n",
      "Epoch [15/20], Step [98/2839], Loss: 0.3527\n",
      "Epoch [15/20], Step [99/2839], Loss: 0.3659\n",
      "Epoch [15/20], Step [100/2839], Loss: 0.8125\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[0.8386839851135596, 0.7033908268859613, 0.44147375819360024, 0.6807064362599435]\n",
      "Epoch [16/20], Step [1/2839], Loss: 1.1142\n",
      "Epoch [16/20], Step [2/2839], Loss: 0.8793\n",
      "Epoch [16/20], Step [3/2839], Loss: 0.4832\n",
      "Epoch [16/20], Step [4/2839], Loss: 0.4152\n",
      "Epoch [16/20], Step [5/2839], Loss: 0.8902\n",
      "Epoch [16/20], Step [6/2839], Loss: 0.3251\n",
      "Epoch [16/20], Step [7/2839], Loss: 0.2732\n",
      "Epoch [16/20], Step [8/2839], Loss: 0.4388\n",
      "Epoch [16/20], Step [9/2839], Loss: 0.7647\n",
      "Epoch [16/20], Step [10/2839], Loss: 0.5901\n",
      "Epoch [16/20], Step [11/2839], Loss: 0.3796\n",
      "Epoch [16/20], Step [12/2839], Loss: 1.6937\n",
      "Epoch [16/20], Step [13/2839], Loss: 0.2483\n",
      "Epoch [16/20], Step [14/2839], Loss: 0.5893\n",
      "Epoch [16/20], Step [15/2839], Loss: 1.8231\n",
      "Epoch [16/20], Step [16/2839], Loss: 0.6260\n",
      "Epoch [16/20], Step [17/2839], Loss: 0.3503\n",
      "Epoch [16/20], Step [18/2839], Loss: 0.7315\n",
      "Epoch [16/20], Step [19/2839], Loss: 0.7520\n",
      "Epoch [16/20], Step [20/2839], Loss: 1.0850\n",
      "Epoch [16/20], Step [21/2839], Loss: 0.7649\n",
      "Epoch [16/20], Step [22/2839], Loss: 0.4101\n",
      "Epoch [16/20], Step [23/2839], Loss: 0.4388\n",
      "Epoch [16/20], Step [24/2839], Loss: 0.1928\n",
      "Epoch [16/20], Step [25/2839], Loss: 0.4797\n",
      "Epoch [16/20], Step [26/2839], Loss: 0.4347\n",
      "Epoch [16/20], Step [27/2839], Loss: 1.4780\n",
      "Epoch [16/20], Step [28/2839], Loss: 0.7933\n",
      "Epoch [16/20], Step [29/2839], Loss: 0.2496\n",
      "Epoch [16/20], Step [30/2839], Loss: 0.4759\n",
      "Epoch [16/20], Step [31/2839], Loss: 0.2296\n",
      "Epoch [16/20], Step [32/2839], Loss: 0.7761\n",
      "Epoch [16/20], Step [33/2839], Loss: 1.5979\n",
      "Epoch [16/20], Step [34/2839], Loss: 0.7450\n",
      "Epoch [16/20], Step [35/2839], Loss: 0.7321\n",
      "Epoch [16/20], Step [36/2839], Loss: 0.4240\n",
      "Epoch [16/20], Step [37/2839], Loss: 0.3095\n",
      "Epoch [16/20], Step [38/2839], Loss: 0.3318\n",
      "Epoch [16/20], Step [39/2839], Loss: 1.0166\n",
      "Epoch [16/20], Step [40/2839], Loss: 0.4061\n",
      "Epoch [16/20], Step [41/2839], Loss: 0.6853\n",
      "Epoch [16/20], Step [42/2839], Loss: 0.8046\n",
      "Epoch [16/20], Step [43/2839], Loss: 0.2883\n",
      "Epoch [16/20], Step [44/2839], Loss: 0.3516\n",
      "Epoch [16/20], Step [45/2839], Loss: 1.2840\n",
      "Epoch [16/20], Step [46/2839], Loss: 0.1987\n",
      "Epoch [16/20], Step [47/2839], Loss: 0.3807\n",
      "Epoch [16/20], Step [48/2839], Loss: 1.2829\n",
      "Epoch [16/20], Step [49/2839], Loss: 0.8097\n",
      "Epoch [16/20], Step [50/2839], Loss: 1.3067\n",
      "Epoch [16/20], Step [51/2839], Loss: 0.3242\n",
      "Epoch [16/20], Step [52/2839], Loss: 0.6037\n",
      "Epoch [16/20], Step [53/2839], Loss: 0.9159\n",
      "Epoch [16/20], Step [54/2839], Loss: 0.6869\n",
      "Epoch [16/20], Step [55/2839], Loss: 0.7789\n",
      "Epoch [16/20], Step [56/2839], Loss: 0.6801\n",
      "Epoch [16/20], Step [57/2839], Loss: 0.4848\n",
      "Epoch [16/20], Step [58/2839], Loss: 0.2938\n",
      "Epoch [16/20], Step [59/2839], Loss: 0.8888\n",
      "Epoch [16/20], Step [60/2839], Loss: 0.2994\n",
      "Epoch [16/20], Step [61/2839], Loss: 0.7024\n",
      "Epoch [16/20], Step [62/2839], Loss: 0.2611\n",
      "Epoch [16/20], Step [63/2839], Loss: 0.4031\n",
      "Epoch [16/20], Step [64/2839], Loss: 0.3409\n",
      "Epoch [16/20], Step [65/2839], Loss: 0.6641\n",
      "Epoch [16/20], Step [66/2839], Loss: 0.8093\n",
      "Epoch [16/20], Step [67/2839], Loss: 0.6080\n",
      "Epoch [16/20], Step [68/2839], Loss: 0.3740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Step [69/2839], Loss: 0.4742\n",
      "Epoch [16/20], Step [70/2839], Loss: 0.5058\n",
      "Epoch [16/20], Step [71/2839], Loss: 0.9202\n",
      "Epoch [16/20], Step [72/2839], Loss: 0.4232\n",
      "Epoch [16/20], Step [73/2839], Loss: 0.4330\n",
      "Epoch [16/20], Step [74/2839], Loss: 1.0039\n",
      "Epoch [16/20], Step [75/2839], Loss: 0.7809\n",
      "Epoch [16/20], Step [76/2839], Loss: 0.5247\n",
      "Epoch [16/20], Step [77/2839], Loss: 1.1064\n",
      "Epoch [16/20], Step [78/2839], Loss: 2.6633\n",
      "Epoch [16/20], Step [79/2839], Loss: 0.5456\n",
      "Epoch [16/20], Step [80/2839], Loss: 1.0323\n",
      "Epoch [16/20], Step [81/2839], Loss: 1.2368\n",
      "Epoch [16/20], Step [82/2839], Loss: 0.9595\n",
      "Epoch [16/20], Step [83/2839], Loss: 0.3411\n",
      "Epoch [16/20], Step [84/2839], Loss: 1.8437\n",
      "Epoch [16/20], Step [85/2839], Loss: 0.7214\n",
      "Epoch [16/20], Step [86/2839], Loss: 0.7851\n",
      "Epoch [16/20], Step [87/2839], Loss: 0.9952\n",
      "Epoch [16/20], Step [88/2839], Loss: 1.2354\n",
      "Epoch [16/20], Step [89/2839], Loss: 0.7573\n",
      "Epoch [16/20], Step [90/2839], Loss: 0.1678\n",
      "Epoch [16/20], Step [91/2839], Loss: 0.7434\n",
      "Epoch [16/20], Step [92/2839], Loss: 0.7324\n",
      "Epoch [16/20], Step [93/2839], Loss: 0.4374\n",
      "Epoch [16/20], Step [94/2839], Loss: 0.9843\n",
      "Epoch [16/20], Step [95/2839], Loss: 0.5270\n",
      "Epoch [16/20], Step [96/2839], Loss: 0.6301\n",
      "Epoch [16/20], Step [97/2839], Loss: 0.2820\n",
      "Epoch [16/20], Step [98/2839], Loss: 0.4019\n",
      "Epoch [16/20], Step [99/2839], Loss: 0.5615\n",
      "Epoch [16/20], Step [100/2839], Loss: 0.3996\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[0.7240424286079413, 0.5242374384244858, 0.47429461801120126, 0.6912981175483777]\n",
      "rmse improved at epoch  16 ; best_mse,best_ci,best_r: 0.5242374384244858 0.6912981175483777 0.47429461801120126\n",
      "Epoch [17/20], Step [1/2839], Loss: 0.7383\n",
      "Epoch [17/20], Step [2/2839], Loss: 0.7880\n",
      "Epoch [17/20], Step [3/2839], Loss: 0.5893\n",
      "Epoch [17/20], Step [4/2839], Loss: 0.5240\n",
      "Epoch [17/20], Step [5/2839], Loss: 0.3215\n",
      "Epoch [17/20], Step [6/2839], Loss: 0.5594\n",
      "Epoch [17/20], Step [7/2839], Loss: 1.0175\n",
      "Epoch [17/20], Step [8/2839], Loss: 0.8170\n",
      "Epoch [17/20], Step [9/2839], Loss: 0.9902\n",
      "Epoch [17/20], Step [10/2839], Loss: 1.6310\n",
      "Epoch [17/20], Step [11/2839], Loss: 0.5953\n",
      "Epoch [17/20], Step [12/2839], Loss: 0.7126\n",
      "Epoch [17/20], Step [13/2839], Loss: 0.1617\n",
      "Epoch [17/20], Step [14/2839], Loss: 0.6338\n",
      "Epoch [17/20], Step [15/2839], Loss: 0.4154\n",
      "Epoch [17/20], Step [16/2839], Loss: 0.6582\n",
      "Epoch [17/20], Step [17/2839], Loss: 0.5820\n",
      "Epoch [17/20], Step [18/2839], Loss: 0.2598\n",
      "Epoch [17/20], Step [19/2839], Loss: 0.6373\n",
      "Epoch [17/20], Step [20/2839], Loss: 1.3082\n",
      "Epoch [17/20], Step [21/2839], Loss: 0.6217\n",
      "Epoch [17/20], Step [22/2839], Loss: 0.3984\n",
      "Epoch [17/20], Step [23/2839], Loss: 0.9990\n",
      "Epoch [17/20], Step [24/2839], Loss: 0.6965\n",
      "Epoch [17/20], Step [25/2839], Loss: 0.4172\n",
      "Epoch [17/20], Step [26/2839], Loss: 1.0591\n",
      "Epoch [17/20], Step [27/2839], Loss: 0.3946\n",
      "Epoch [17/20], Step [28/2839], Loss: 0.9717\n",
      "Epoch [17/20], Step [29/2839], Loss: 0.6992\n",
      "Epoch [17/20], Step [30/2839], Loss: 0.8299\n",
      "Epoch [17/20], Step [31/2839], Loss: 0.3552\n",
      "Epoch [17/20], Step [32/2839], Loss: 0.6129\n",
      "Epoch [17/20], Step [33/2839], Loss: 0.8283\n",
      "Epoch [17/20], Step [34/2839], Loss: 0.4017\n",
      "Epoch [17/20], Step [35/2839], Loss: 0.3300\n",
      "Epoch [17/20], Step [36/2839], Loss: 0.5922\n",
      "Epoch [17/20], Step [37/2839], Loss: 0.5389\n",
      "Epoch [17/20], Step [38/2839], Loss: 0.5677\n",
      "Epoch [17/20], Step [39/2839], Loss: 0.5371\n",
      "Epoch [17/20], Step [40/2839], Loss: 0.6314\n",
      "Epoch [17/20], Step [41/2839], Loss: 0.4138\n",
      "Epoch [17/20], Step [42/2839], Loss: 0.2745\n",
      "Epoch [17/20], Step [43/2839], Loss: 0.9466\n",
      "Epoch [17/20], Step [44/2839], Loss: 0.3446\n",
      "Epoch [17/20], Step [45/2839], Loss: 0.6460\n",
      "Epoch [17/20], Step [46/2839], Loss: 0.3908\n",
      "Epoch [17/20], Step [47/2839], Loss: 0.5311\n",
      "Epoch [17/20], Step [48/2839], Loss: 0.6870\n",
      "Epoch [17/20], Step [49/2839], Loss: 0.2408\n",
      "Epoch [17/20], Step [50/2839], Loss: 0.4497\n",
      "Epoch [17/20], Step [51/2839], Loss: 0.4129\n",
      "Epoch [17/20], Step [52/2839], Loss: 0.3415\n",
      "Epoch [17/20], Step [53/2839], Loss: 0.7518\n",
      "Epoch [17/20], Step [54/2839], Loss: 0.3683\n",
      "Epoch [17/20], Step [55/2839], Loss: 0.3026\n",
      "Epoch [17/20], Step [56/2839], Loss: 0.1902\n",
      "Epoch [17/20], Step [57/2839], Loss: 0.5858\n",
      "Epoch [17/20], Step [58/2839], Loss: 0.3331\n",
      "Epoch [17/20], Step [59/2839], Loss: 0.4378\n",
      "Epoch [17/20], Step [60/2839], Loss: 0.4883\n",
      "Epoch [17/20], Step [61/2839], Loss: 0.9960\n",
      "Epoch [17/20], Step [62/2839], Loss: 0.1911\n",
      "Epoch [17/20], Step [63/2839], Loss: 0.5019\n",
      "Epoch [17/20], Step [64/2839], Loss: 1.0704\n",
      "Epoch [17/20], Step [65/2839], Loss: 0.2524\n",
      "Epoch [17/20], Step [66/2839], Loss: 0.2832\n",
      "Epoch [17/20], Step [67/2839], Loss: 0.3185\n",
      "Epoch [17/20], Step [68/2839], Loss: 0.6495\n",
      "Epoch [17/20], Step [69/2839], Loss: 0.3912\n",
      "Epoch [17/20], Step [70/2839], Loss: 0.9970\n",
      "Epoch [17/20], Step [71/2839], Loss: 0.8366\n",
      "Epoch [17/20], Step [72/2839], Loss: 0.7745\n",
      "Epoch [17/20], Step [73/2839], Loss: 0.7590\n",
      "Epoch [17/20], Step [74/2839], Loss: 0.2883\n",
      "Epoch [17/20], Step [75/2839], Loss: 0.4679\n",
      "Epoch [17/20], Step [76/2839], Loss: 0.8264\n",
      "Epoch [17/20], Step [77/2839], Loss: 0.8470\n",
      "Epoch [17/20], Step [78/2839], Loss: 0.9689\n",
      "Epoch [17/20], Step [79/2839], Loss: 1.1367\n",
      "Epoch [17/20], Step [80/2839], Loss: 0.5211\n",
      "Epoch [17/20], Step [81/2839], Loss: 0.5506\n",
      "Epoch [17/20], Step [82/2839], Loss: 1.3392\n",
      "Epoch [17/20], Step [83/2839], Loss: 0.6132\n",
      "Epoch [17/20], Step [84/2839], Loss: 0.3442\n",
      "Epoch [17/20], Step [85/2839], Loss: 1.2583\n",
      "Epoch [17/20], Step [86/2839], Loss: 0.5727\n",
      "Epoch [17/20], Step [87/2839], Loss: 0.4737\n",
      "Epoch [17/20], Step [88/2839], Loss: 0.7892\n",
      "Epoch [17/20], Step [89/2839], Loss: 0.4081\n",
      "Epoch [17/20], Step [90/2839], Loss: 0.7967\n",
      "Epoch [17/20], Step [91/2839], Loss: 0.6699\n",
      "Epoch [17/20], Step [92/2839], Loss: 0.4392\n",
      "Epoch [17/20], Step [93/2839], Loss: 0.8797\n",
      "Epoch [17/20], Step [94/2839], Loss: 0.4871\n",
      "Epoch [17/20], Step [95/2839], Loss: 1.0457\n",
      "Epoch [17/20], Step [96/2839], Loss: 0.9630\n",
      "Epoch [17/20], Step [97/2839], Loss: 0.5029\n",
      "Epoch [17/20], Step [98/2839], Loss: 0.6143\n",
      "Epoch [17/20], Step [99/2839], Loss: 0.9988\n",
      "Epoch [17/20], Step [100/2839], Loss: 0.3685\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[0.7346808468037107, 0.5397559466602174, 0.5211351987512481, 0.692018022734618]\n",
      "Epoch [18/20], Step [1/2839], Loss: 0.4700\n",
      "Epoch [18/20], Step [2/2839], Loss: 0.8980\n",
      "Epoch [18/20], Step [3/2839], Loss: 0.5573\n",
      "Epoch [18/20], Step [4/2839], Loss: 0.4367\n",
      "Epoch [18/20], Step [5/2839], Loss: 0.7592\n",
      "Epoch [18/20], Step [6/2839], Loss: 0.4312\n",
      "Epoch [18/20], Step [7/2839], Loss: 0.5927\n",
      "Epoch [18/20], Step [8/2839], Loss: 0.3463\n",
      "Epoch [18/20], Step [9/2839], Loss: 0.3200\n",
      "Epoch [18/20], Step [10/2839], Loss: 0.6514\n",
      "Epoch [18/20], Step [11/2839], Loss: 0.3097\n",
      "Epoch [18/20], Step [12/2839], Loss: 0.3780\n",
      "Epoch [18/20], Step [13/2839], Loss: 0.5748\n",
      "Epoch [18/20], Step [14/2839], Loss: 0.5646\n",
      "Epoch [18/20], Step [15/2839], Loss: 0.6778\n",
      "Epoch [18/20], Step [16/2839], Loss: 0.5983\n",
      "Epoch [18/20], Step [17/2839], Loss: 0.4157\n",
      "Epoch [18/20], Step [18/2839], Loss: 0.2258\n",
      "Epoch [18/20], Step [19/2839], Loss: 0.4447\n",
      "Epoch [18/20], Step [20/2839], Loss: 0.7054\n",
      "Epoch [18/20], Step [21/2839], Loss: 0.4556\n",
      "Epoch [18/20], Step [22/2839], Loss: 0.6263\n",
      "Epoch [18/20], Step [23/2839], Loss: 0.5079\n",
      "Epoch [18/20], Step [24/2839], Loss: 0.3554\n",
      "Epoch [18/20], Step [25/2839], Loss: 0.5631\n",
      "Epoch [18/20], Step [26/2839], Loss: 0.7260\n",
      "Epoch [18/20], Step [27/2839], Loss: 0.3165\n",
      "Epoch [18/20], Step [28/2839], Loss: 0.6422\n",
      "Epoch [18/20], Step [29/2839], Loss: 0.8846\n",
      "Epoch [18/20], Step [30/2839], Loss: 0.7125\n",
      "Epoch [18/20], Step [31/2839], Loss: 0.8926\n",
      "Epoch [18/20], Step [32/2839], Loss: 0.5582\n",
      "Epoch [18/20], Step [33/2839], Loss: 0.9793\n",
      "Epoch [18/20], Step [34/2839], Loss: 0.6074\n",
      "Epoch [18/20], Step [35/2839], Loss: 0.8208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Step [36/2839], Loss: 0.5620\n",
      "Epoch [18/20], Step [37/2839], Loss: 0.4737\n",
      "Epoch [18/20], Step [38/2839], Loss: 0.4954\n",
      "Epoch [18/20], Step [39/2839], Loss: 0.7039\n",
      "Epoch [18/20], Step [40/2839], Loss: 0.7808\n",
      "Epoch [18/20], Step [41/2839], Loss: 1.0578\n",
      "Epoch [18/20], Step [42/2839], Loss: 0.7608\n",
      "Epoch [18/20], Step [43/2839], Loss: 0.6503\n",
      "Epoch [18/20], Step [44/2839], Loss: 0.8963\n",
      "Epoch [18/20], Step [45/2839], Loss: 0.4349\n",
      "Epoch [18/20], Step [46/2839], Loss: 0.9476\n",
      "Epoch [18/20], Step [47/2839], Loss: 0.2222\n",
      "Epoch [18/20], Step [48/2839], Loss: 1.4604\n",
      "Epoch [18/20], Step [49/2839], Loss: 0.4576\n",
      "Epoch [18/20], Step [50/2839], Loss: 0.4455\n",
      "Epoch [18/20], Step [51/2839], Loss: 0.4531\n",
      "Epoch [18/20], Step [52/2839], Loss: 1.0076\n",
      "Epoch [18/20], Step [53/2839], Loss: 1.1397\n",
      "Epoch [18/20], Step [54/2839], Loss: 0.4643\n",
      "Epoch [18/20], Step [55/2839], Loss: 0.4215\n",
      "Epoch [18/20], Step [56/2839], Loss: 0.7645\n",
      "Epoch [18/20], Step [57/2839], Loss: 0.3848\n",
      "Epoch [18/20], Step [58/2839], Loss: 0.4553\n",
      "Epoch [18/20], Step [59/2839], Loss: 0.8161\n",
      "Epoch [18/20], Step [60/2839], Loss: 0.7046\n",
      "Epoch [18/20], Step [61/2839], Loss: 0.4877\n",
      "Epoch [18/20], Step [62/2839], Loss: 0.8905\n",
      "Epoch [18/20], Step [63/2839], Loss: 0.3904\n",
      "Epoch [18/20], Step [64/2839], Loss: 0.7733\n",
      "Epoch [18/20], Step [65/2839], Loss: 0.3025\n",
      "Epoch [18/20], Step [66/2839], Loss: 0.5437\n",
      "Epoch [18/20], Step [67/2839], Loss: 0.7267\n",
      "Epoch [18/20], Step [68/2839], Loss: 0.6263\n",
      "Epoch [18/20], Step [69/2839], Loss: 0.7292\n",
      "Epoch [18/20], Step [70/2839], Loss: 0.4842\n",
      "Epoch [18/20], Step [71/2839], Loss: 0.8288\n",
      "Epoch [18/20], Step [72/2839], Loss: 1.4932\n",
      "Epoch [18/20], Step [73/2839], Loss: 0.6881\n",
      "Epoch [18/20], Step [74/2839], Loss: 0.7587\n",
      "Epoch [18/20], Step [75/2839], Loss: 1.0220\n",
      "Epoch [18/20], Step [76/2839], Loss: 0.4886\n",
      "Epoch [18/20], Step [77/2839], Loss: 0.7235\n",
      "Epoch [18/20], Step [78/2839], Loss: 1.2139\n",
      "Epoch [18/20], Step [79/2839], Loss: 0.7410\n",
      "Epoch [18/20], Step [80/2839], Loss: 0.7484\n",
      "Epoch [18/20], Step [81/2839], Loss: 0.6235\n",
      "Epoch [18/20], Step [82/2839], Loss: 0.8631\n",
      "Epoch [18/20], Step [83/2839], Loss: 0.5923\n",
      "Epoch [18/20], Step [84/2839], Loss: 0.8185\n",
      "Epoch [18/20], Step [85/2839], Loss: 0.4063\n",
      "Epoch [18/20], Step [86/2839], Loss: 1.1786\n",
      "Epoch [18/20], Step [87/2839], Loss: 0.2011\n",
      "Epoch [18/20], Step [88/2839], Loss: 0.5461\n",
      "Epoch [18/20], Step [89/2839], Loss: 0.3315\n",
      "Epoch [18/20], Step [90/2839], Loss: 0.3079\n",
      "Epoch [18/20], Step [91/2839], Loss: 0.2428\n",
      "Epoch [18/20], Step [92/2839], Loss: 1.0593\n",
      "Epoch [18/20], Step [93/2839], Loss: 0.5275\n",
      "Epoch [18/20], Step [94/2839], Loss: 0.2209\n",
      "Epoch [18/20], Step [95/2839], Loss: 0.6057\n",
      "Epoch [18/20], Step [96/2839], Loss: 0.9443\n",
      "Epoch [18/20], Step [97/2839], Loss: 1.0130\n",
      "Epoch [18/20], Step [98/2839], Loss: 0.7974\n",
      "Epoch [18/20], Step [99/2839], Loss: 0.7894\n",
      "Epoch [18/20], Step [100/2839], Loss: 0.3779\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[0.8222283606708584, 0.6760594770914872, 0.5231035665460185, 0.6950151661519975]\n",
      "Epoch [19/20], Step [1/2839], Loss: 0.5970\n",
      "Epoch [19/20], Step [2/2839], Loss: 0.6645\n",
      "Epoch [19/20], Step [3/2839], Loss: 0.1862\n",
      "Epoch [19/20], Step [4/2839], Loss: 0.6431\n",
      "Epoch [19/20], Step [5/2839], Loss: 0.5399\n",
      "Epoch [19/20], Step [6/2839], Loss: 0.4429\n",
      "Epoch [19/20], Step [7/2839], Loss: 0.7685\n",
      "Epoch [19/20], Step [8/2839], Loss: 1.1944\n",
      "Epoch [19/20], Step [9/2839], Loss: 0.7085\n",
      "Epoch [19/20], Step [10/2839], Loss: 1.0366\n",
      "Epoch [19/20], Step [11/2839], Loss: 0.9274\n",
      "Epoch [19/20], Step [12/2839], Loss: 0.3652\n",
      "Epoch [19/20], Step [13/2839], Loss: 0.2311\n",
      "Epoch [19/20], Step [14/2839], Loss: 0.4335\n",
      "Epoch [19/20], Step [15/2839], Loss: 0.6263\n",
      "Epoch [19/20], Step [16/2839], Loss: 0.5455\n",
      "Epoch [19/20], Step [17/2839], Loss: 0.5038\n",
      "Epoch [19/20], Step [18/2839], Loss: 0.5629\n",
      "Epoch [19/20], Step [19/2839], Loss: 0.5796\n",
      "Epoch [19/20], Step [20/2839], Loss: 0.3197\n",
      "Epoch [19/20], Step [21/2839], Loss: 0.6605\n",
      "Epoch [19/20], Step [22/2839], Loss: 0.2489\n",
      "Epoch [19/20], Step [23/2839], Loss: 0.6959\n",
      "Epoch [19/20], Step [24/2839], Loss: 0.4659\n",
      "Epoch [19/20], Step [25/2839], Loss: 0.5024\n",
      "Epoch [19/20], Step [26/2839], Loss: 0.4492\n",
      "Epoch [19/20], Step [27/2839], Loss: 0.2039\n",
      "Epoch [19/20], Step [28/2839], Loss: 0.4297\n",
      "Epoch [19/20], Step [29/2839], Loss: 0.7666\n",
      "Epoch [19/20], Step [30/2839], Loss: 0.6286\n",
      "Epoch [19/20], Step [31/2839], Loss: 0.3925\n",
      "Epoch [19/20], Step [32/2839], Loss: 0.2005\n",
      "Epoch [19/20], Step [33/2839], Loss: 0.6742\n",
      "Epoch [19/20], Step [34/2839], Loss: 0.5647\n",
      "Epoch [19/20], Step [35/2839], Loss: 0.6830\n",
      "Epoch [19/20], Step [36/2839], Loss: 0.2639\n",
      "Epoch [19/20], Step [37/2839], Loss: 0.3929\n",
      "Epoch [19/20], Step [38/2839], Loss: 0.7293\n",
      "Epoch [19/20], Step [39/2839], Loss: 0.2436\n",
      "Epoch [19/20], Step [40/2839], Loss: 0.5295\n",
      "Epoch [19/20], Step [41/2839], Loss: 0.5464\n",
      "Epoch [19/20], Step [42/2839], Loss: 0.3593\n",
      "Epoch [19/20], Step [43/2839], Loss: 0.6880\n",
      "Epoch [19/20], Step [44/2839], Loss: 0.4201\n",
      "Epoch [19/20], Step [45/2839], Loss: 0.7648\n",
      "Epoch [19/20], Step [46/2839], Loss: 0.9379\n",
      "Epoch [19/20], Step [47/2839], Loss: 0.7579\n",
      "Epoch [19/20], Step [48/2839], Loss: 0.3313\n",
      "Epoch [19/20], Step [49/2839], Loss: 0.4243\n",
      "Epoch [19/20], Step [50/2839], Loss: 0.7396\n",
      "Epoch [19/20], Step [51/2839], Loss: 0.8957\n",
      "Epoch [19/20], Step [52/2839], Loss: 0.5820\n",
      "Epoch [19/20], Step [53/2839], Loss: 0.9420\n",
      "Epoch [19/20], Step [54/2839], Loss: 1.6813\n",
      "Epoch [19/20], Step [55/2839], Loss: 0.6829\n",
      "Epoch [19/20], Step [56/2839], Loss: 0.7990\n",
      "Epoch [19/20], Step [57/2839], Loss: 1.3099\n",
      "Epoch [19/20], Step [58/2839], Loss: 1.3638\n",
      "Epoch [19/20], Step [59/2839], Loss: 1.1355\n",
      "Epoch [19/20], Step [60/2839], Loss: 1.5653\n",
      "Epoch [19/20], Step [61/2839], Loss: 1.0541\n",
      "Epoch [19/20], Step [62/2839], Loss: 1.2179\n",
      "Epoch [19/20], Step [63/2839], Loss: 0.4018\n",
      "Epoch [19/20], Step [64/2839], Loss: 0.9270\n",
      "Epoch [19/20], Step [65/2839], Loss: 0.9095\n",
      "Epoch [19/20], Step [66/2839], Loss: 1.2401\n",
      "Epoch [19/20], Step [67/2839], Loss: 0.9203\n",
      "Epoch [19/20], Step [68/2839], Loss: 0.7981\n",
      "Epoch [19/20], Step [69/2839], Loss: 0.2180\n",
      "Epoch [19/20], Step [70/2839], Loss: 0.9049\n",
      "Epoch [19/20], Step [71/2839], Loss: 1.3092\n",
      "Epoch [19/20], Step [72/2839], Loss: 0.6230\n",
      "Epoch [19/20], Step [73/2839], Loss: 0.7095\n",
      "Epoch [19/20], Step [74/2839], Loss: 1.0906\n",
      "Epoch [19/20], Step [75/2839], Loss: 0.3684\n",
      "Epoch [19/20], Step [76/2839], Loss: 1.1169\n",
      "Epoch [19/20], Step [77/2839], Loss: 0.6804\n",
      "Epoch [19/20], Step [78/2839], Loss: 0.3364\n",
      "Epoch [19/20], Step [79/2839], Loss: 0.7058\n",
      "Epoch [19/20], Step [80/2839], Loss: 0.3798\n",
      "Epoch [19/20], Step [81/2839], Loss: 0.7042\n",
      "Epoch [19/20], Step [82/2839], Loss: 0.3341\n",
      "Epoch [19/20], Step [83/2839], Loss: 0.3197\n",
      "Epoch [19/20], Step [84/2839], Loss: 0.2944\n",
      "Epoch [19/20], Step [85/2839], Loss: 0.3866\n",
      "Epoch [19/20], Step [86/2839], Loss: 0.3394\n",
      "Epoch [19/20], Step [87/2839], Loss: 0.7104\n",
      "Epoch [19/20], Step [88/2839], Loss: 0.2622\n",
      "Epoch [19/20], Step [89/2839], Loss: 5.2955\n",
      "Epoch [19/20], Step [90/2839], Loss: 0.3838\n",
      "Epoch [19/20], Step [91/2839], Loss: 0.8924\n",
      "Epoch [19/20], Step [92/2839], Loss: 0.5855\n",
      "Epoch [19/20], Step [93/2839], Loss: 0.3643\n",
      "Epoch [19/20], Step [94/2839], Loss: 0.6464\n",
      "Epoch [19/20], Step [95/2839], Loss: 0.4322\n",
      "Epoch [19/20], Step [96/2839], Loss: 0.9012\n",
      "Epoch [19/20], Step [97/2839], Loss: 0.5215\n",
      "Epoch [19/20], Step [98/2839], Loss: 0.3321\n",
      "Epoch [19/20], Step [99/2839], Loss: 0.2772\n",
      "Epoch [19/20], Step [100/2839], Loss: 1.0647\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[0.7131424133060602, 0.5085721016559915, 0.5501018035381691, 0.7107378353693552]\n",
      "rmse improved at epoch  19 ; best_mse,best_ci,best_r: 0.5085721016559915 0.7107378353693552 0.5501018035381691\n",
      "Epoch [20/20], Step [1/2839], Loss: 0.5349\n",
      "Epoch [20/20], Step [2/2839], Loss: 0.5284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Step [3/2839], Loss: 0.4747\n",
      "Epoch [20/20], Step [4/2839], Loss: 0.7698\n",
      "Epoch [20/20], Step [5/2839], Loss: 0.3076\n",
      "Epoch [20/20], Step [6/2839], Loss: 0.6290\n",
      "Epoch [20/20], Step [7/2839], Loss: 0.6189\n",
      "Epoch [20/20], Step [8/2839], Loss: 0.3992\n",
      "Epoch [20/20], Step [9/2839], Loss: 0.2148\n",
      "Epoch [20/20], Step [10/2839], Loss: 0.7674\n",
      "Epoch [20/20], Step [11/2839], Loss: 0.8013\n",
      "Epoch [20/20], Step [12/2839], Loss: 0.2933\n",
      "Epoch [20/20], Step [13/2839], Loss: 0.2562\n",
      "Epoch [20/20], Step [14/2839], Loss: 0.3423\n",
      "Epoch [20/20], Step [15/2839], Loss: 0.4720\n",
      "Epoch [20/20], Step [16/2839], Loss: 0.6024\n",
      "Epoch [20/20], Step [17/2839], Loss: 0.3210\n",
      "Epoch [20/20], Step [18/2839], Loss: 0.4159\n",
      "Epoch [20/20], Step [19/2839], Loss: 0.1841\n",
      "Epoch [20/20], Step [20/2839], Loss: 1.7063\n",
      "Epoch [20/20], Step [21/2839], Loss: 0.5621\n",
      "Epoch [20/20], Step [22/2839], Loss: 0.8905\n",
      "Epoch [20/20], Step [23/2839], Loss: 0.6391\n",
      "Epoch [20/20], Step [24/2839], Loss: 0.3622\n",
      "Epoch [20/20], Step [25/2839], Loss: 0.7734\n",
      "Epoch [20/20], Step [26/2839], Loss: 1.4989\n",
      "Epoch [20/20], Step [27/2839], Loss: 0.2626\n",
      "Epoch [20/20], Step [28/2839], Loss: 0.4691\n",
      "Epoch [20/20], Step [29/2839], Loss: 0.7133\n",
      "Epoch [20/20], Step [30/2839], Loss: 0.7183\n",
      "Epoch [20/20], Step [31/2839], Loss: 0.8499\n",
      "Epoch [20/20], Step [32/2839], Loss: 0.1156\n",
      "Epoch [20/20], Step [33/2839], Loss: 1.0550\n",
      "Epoch [20/20], Step [34/2839], Loss: 0.1931\n",
      "Epoch [20/20], Step [35/2839], Loss: 0.5631\n",
      "Epoch [20/20], Step [36/2839], Loss: 0.7361\n",
      "Epoch [20/20], Step [37/2839], Loss: 0.2364\n",
      "Epoch [20/20], Step [38/2839], Loss: 0.5141\n",
      "Epoch [20/20], Step [39/2839], Loss: 0.9600\n",
      "Epoch [20/20], Step [40/2839], Loss: 0.5316\n",
      "Epoch [20/20], Step [41/2839], Loss: 0.4013\n",
      "Epoch [20/20], Step [42/2839], Loss: 0.6422\n",
      "Epoch [20/20], Step [43/2839], Loss: 0.7521\n",
      "Epoch [20/20], Step [44/2839], Loss: 0.5874\n",
      "Epoch [20/20], Step [45/2839], Loss: 0.4456\n",
      "Epoch [20/20], Step [46/2839], Loss: 0.4059\n",
      "Epoch [20/20], Step [47/2839], Loss: 0.6469\n",
      "Epoch [20/20], Step [48/2839], Loss: 0.1536\n",
      "Epoch [20/20], Step [49/2839], Loss: 0.5792\n",
      "Epoch [20/20], Step [50/2839], Loss: 0.5332\n",
      "Epoch [20/20], Step [51/2839], Loss: 0.4888\n",
      "Epoch [20/20], Step [52/2839], Loss: 0.2053\n",
      "Epoch [20/20], Step [53/2839], Loss: 0.4083\n",
      "Epoch [20/20], Step [54/2839], Loss: 0.3009\n",
      "Epoch [20/20], Step [55/2839], Loss: 0.3032\n",
      "Epoch [20/20], Step [56/2839], Loss: 0.6515\n",
      "Epoch [20/20], Step [57/2839], Loss: 0.9621\n",
      "Epoch [20/20], Step [58/2839], Loss: 0.1725\n",
      "Epoch [20/20], Step [59/2839], Loss: 0.2120\n",
      "Epoch [20/20], Step [60/2839], Loss: 0.4456\n",
      "Epoch [20/20], Step [61/2839], Loss: 0.2451\n",
      "Epoch [20/20], Step [62/2839], Loss: 0.5222\n",
      "Epoch [20/20], Step [63/2839], Loss: 1.0359\n",
      "Epoch [20/20], Step [64/2839], Loss: 0.3885\n",
      "Epoch [20/20], Step [65/2839], Loss: 0.3840\n",
      "Epoch [20/20], Step [66/2839], Loss: 1.1907\n",
      "Epoch [20/20], Step [67/2839], Loss: 1.4275\n",
      "Epoch [20/20], Step [68/2839], Loss: 0.2153\n",
      "Epoch [20/20], Step [69/2839], Loss: 0.3129\n",
      "Epoch [20/20], Step [70/2839], Loss: 0.4161\n",
      "Epoch [20/20], Step [71/2839], Loss: 0.6800\n",
      "Epoch [20/20], Step [72/2839], Loss: 0.5018\n",
      "Epoch [20/20], Step [73/2839], Loss: 0.6830\n",
      "Epoch [20/20], Step [74/2839], Loss: 1.4250\n",
      "Epoch [20/20], Step [75/2839], Loss: 0.4921\n",
      "Epoch [20/20], Step [76/2839], Loss: 0.6929\n",
      "Epoch [20/20], Step [77/2839], Loss: 1.1406\n",
      "Epoch [20/20], Step [78/2839], Loss: 0.7972\n",
      "Epoch [20/20], Step [79/2839], Loss: 0.4836\n",
      "Epoch [20/20], Step [80/2839], Loss: 0.4055\n",
      "Epoch [20/20], Step [81/2839], Loss: 0.6514\n",
      "Epoch [20/20], Step [82/2839], Loss: 0.6593\n",
      "Epoch [20/20], Step [83/2839], Loss: 0.4396\n",
      "Epoch [20/20], Step [84/2839], Loss: 0.8078\n",
      "Epoch [20/20], Step [85/2839], Loss: 0.5910\n",
      "Epoch [20/20], Step [86/2839], Loss: 0.5250\n",
      "Epoch [20/20], Step [87/2839], Loss: 0.3503\n",
      "Epoch [20/20], Step [88/2839], Loss: 0.3627\n",
      "Epoch [20/20], Step [89/2839], Loss: 0.5036\n",
      "Epoch [20/20], Step [90/2839], Loss: 0.2151\n",
      "Epoch [20/20], Step [91/2839], Loss: 0.4788\n",
      "Epoch [20/20], Step [92/2839], Loss: 0.6750\n",
      "Epoch [20/20], Step [93/2839], Loss: 0.1894\n",
      "Epoch [20/20], Step [94/2839], Loss: 0.4592\n",
      "Epoch [20/20], Step [95/2839], Loss: 0.2415\n",
      "Epoch [20/20], Step [96/2839], Loss: 0.8336\n",
      "Epoch [20/20], Step [97/2839], Loss: 1.3237\n",
      "Epoch [20/20], Step [98/2839], Loss: 0.7469\n",
      "Epoch [20/20], Step [99/2839], Loss: 0.8437\n",
      "Epoch [20/20], Step [100/2839], Loss: 0.4539\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "[0.6989927421224305, 0.4885908535398346, 0.5726389704157445, 0.7137168741613799]\n",
      "rmse improved at epoch  20 ; best_mse,best_ci,best_r: 0.4885908535398346 0.7137168741613799 0.5726389704157445\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "best_mse = 1000\n",
    "best_ci = 0\n",
    "# model_file_name = 'best_sim-CNN-DTA_kiba.model'\n",
    "# result_file_name = 'best_result_sim-CNNDTA_kiba.csv'\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    c=0\n",
    "    for i in train_loader:\n",
    "        c=c+1\n",
    "        if (c > 100):\n",
    "            break\n",
    "        images = i['outer_product']\n",
    "        labels = i['Label']\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs.flatten(), labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "           \n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "               .format(epoch+1, num_epochs, c, total_step, loss.item()))\n",
    "    \n",
    "#     taking best model so far\n",
    "    G,P = predicting(model, device, train_loader)\n",
    "    ret = [rmse(G, P), mse(G, P), pearson(G, P), ci(G, P)]\n",
    "    print(ret)\n",
    "    if ret[1] < best_mse:\n",
    "#         torch.save(model.state_dict(), model_file_name)\n",
    "#         with open(result_file_name, 'w') as f:\n",
    "#             f.write(','.join(map(str, ret)))\n",
    "        best_epoch = epoch+1\n",
    "        best_mse = ret[1]\n",
    "        best_ci = ret[-1]\n",
    "        best_r = ret[2]\n",
    "        \n",
    "        print('rmse improved at epoch ', best_epoch,\n",
    "                      '; best_mse,best_ci,best_r:', best_mse, best_ci,best_r)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833bf3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d4b7a9f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ConvNet:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([128, 342324]) from checkpoint, the shape in current model is torch.Size([128, 304722]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8577/176357047.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./best_sim-CNN-DTA_kiba_fold1NEW.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# model.eval()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtotal_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtotal_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/geometric/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1407\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ConvNet:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([128, 342324]) from checkpoint, the shape in current model is torch.Size([128, 304722])."
     ]
    }
   ],
   "source": [
    "model = ConvNet().to(device)\n",
    "model.load_state_dict(torch.load('./best_sim-CNN-DTA_kiba_fold1NEW.model'))\n",
    "# model.eval()\n",
    "total_preds = np.array([])\n",
    "total_labels = np.array([])\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    c=0\n",
    "    for i in train_loader:\n",
    "        print(c)\n",
    "        c=c+1\n",
    "        if(c==100):\n",
    "            break\n",
    "        images = i['outer_product']\n",
    "        labels = i['Label']\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images) \n",
    "        outputs = outputs.cpu().detach().numpy().flatten()\n",
    "        labels =labels.cpu().detach().numpy().flatten()\n",
    "        total_preds = np.concatenate([total_preds, outputs])\n",
    "        total_labels = np.concatenate([total_labels, labels])\n",
    "#         total_preds = torch.cat(total_preds, outputs.cpu(), 0 )\n",
    "#         total_labels = torch.cat(total_labels, labels.cpu(), 0)\n",
    "#         break\n",
    "\n",
    "G,P = total_labels, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE = \",mse(G,P),flush=True)\n",
    "print(\"R = \",pearson(G,P),flush=True)\n",
    "print(\"CI = \",ci(G,P),flush=True)\n",
    "print(\"RMSE = \",rmse(G,P),flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ff4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f23a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./best_sim-CNN-DTA_kiba_fold0NEW.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b708d822",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0647522a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# G,P = predicting(model, device, test_loader)\n",
    "# it changes to trainmode again()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3854a24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_preds = np.array([])\n",
    "total_labels = np.array([])\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    c=0\n",
    "    for i in test_loader:\n",
    "        print(c)\n",
    "        c=c+1\n",
    "        images = i['outer_product']\n",
    "        labels = i['Label']\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images) \n",
    "        outputs = outputs.cpu().detach().numpy().flatten()\n",
    "        labels =labels.cpu().detach().numpy().flatten()\n",
    "        total_preds = np.concatenate([total_preds, outputs])\n",
    "        total_labels = np.concatenate([total_labels, labels])\n",
    "#         total_preds = torch.cat(total_preds, outputs.cpu(), 0 )\n",
    "#         total_labels = torch.cat(total_labels, labels.cpu(), 0)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030531a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "G,P = total_labels, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd214f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE = \",mse(G,P))\n",
    "print(\"R = \",pearson(G,P))\n",
    "print(\"CI = \",ci(G,P))\n",
    "print(\"RMSE = \",rmse(G,P))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ebb42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953ed35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c78110e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
