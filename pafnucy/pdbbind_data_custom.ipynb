{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "\n",
    "\n",
    "import pybel\n",
    "from tfbio.data import Featurizer\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the extracted PDBbind dataset\n",
    "path = '/scratch/kanakala.ganesh/PDBBIND/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/scratch/kanakala.ganesh/PDBBIND/refined-set/index/INDEX_general_PL_data.2019')\n",
    "lines = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = open('affinity_data_custom.csv', 'w')\n",
    "f2.write('pdbid -logKd/Ki' + '\\n')\n",
    "for i in range(len(lines)):\n",
    "    l = lines[i].split()\n",
    "    if (l[0] != \"#\"):\n",
    "        f2.write(l[0] + ' '+ l[3] + '\\n')\n",
    "    else:\n",
    "        print(lines[i])\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/scratch/kanakala.ganesh/PDBBIND/v2019-other-PL/'\n",
    "path2 = '/scratch/kanakala.ganesh/PDBBIND/refined-set/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_list = os.listdir(path1)\n",
    "refined_list = os.listdir(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(general_list), len(refined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f3 = open('affinity_data_custom_clean.csv', 'w')\n",
    "f3.write('pdbid -logKd/Ki class' + '\\n')\n",
    "c=0\n",
    "for i in range(len(lines)):\n",
    "    l = lines[i].split()\n",
    "    if (l[0] != \"#\"):\n",
    "        id = l[0]\n",
    "        if id in general_list and id not in refined_list:\n",
    "            f3.write(l[0] + ' '+ l[3] + ' ' +  'general'+'\\n')\n",
    "        elif id in refined_list and id not in general_list:\n",
    "            f3.write(l[0] + ' '+ l[3] + ' ' + 'refined'+'\\n')\n",
    "        else:\n",
    "            print(lines[i], \"missing\")\n",
    "            print(c)\n",
    "            c=c+1\n",
    "    else:\n",
    "        print(lines[i])\n",
    "f3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path3= '/scratch/kanakala.ganesh/PDBBIND/CORE-FULL/ALL-CORE-affinities.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_list_df = pd.read_csv(path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_list = core_list_df['id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_data = pd.read_csv('affinity_data_custom_clean.csv', delimiter=' ')\n",
    "# affinity_data = affinity_data[~np.in1d(affinity_data['pdbid'], list(missing))]\n",
    "affinity_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaNs\n",
    "\n",
    "affinity_data['-logKd/Ki'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in core_list:\n",
    "    if i in general_list:\n",
    "        general_list.remove(i)\n",
    "    if i in refined_list:\n",
    "        refined_list.remove(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate core, refined, and general sets\n",
    "\n",
    "# core_set = ! grep -v '#' $path/PDBbind_2016_plain_text_index/index/INDEX_core_data.2016 | cut -f 1 -d ' '\n",
    "# core_set = set(core_set)\n",
    "\n",
    "# refined_set = ! grep -v '#' $path/PDBbind_2016_plain_text_index/index/INDEX_refined_data.2016 | cut -f 1 -d ' '\n",
    "# refined_set = set(refined_set)\n",
    "\n",
    "# general_set = set(affinity_data['pdbid'])\n",
    "\n",
    "\n",
    "# assert core_set & refined_set == core_set\n",
    "# assert refined_set & general_set == refined_set\n",
    "\n",
    "# len(general_set), len(refined_set), len(core_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_data['include'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exclude v 2013 core set - it will be used as another test set\n",
    "\n",
    "# core2013 = ! cat core_pdbbind2013.ids\n",
    "# core2013 = set(core2013)\n",
    "\n",
    "# affinity_data['include'] = True\n",
    "# affinity_data.loc[np.in1d(affinity_data['pdbid'], list(core2013 & (general_set - core_set))), 'include'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_data['set'] = affinity_data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_data = affinity_data.drop(columns=['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "affinity_data.loc[np.in1d(affinity_data['pdbid'], list(general_list)), 'set'] = 'general'\n",
    "\n",
    "affinity_data.loc[np.in1d(affinity_data['pdbid'], list(refined_list)), 'set'] = 'refined'\n",
    "\n",
    "affinity_data.loc[np.in1d(affinity_data['pdbid'], list(core_list)), 'set'] = 'core'\n",
    "\n",
    "affinity_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_data[affinity_data['include']].groupby('set').apply(len).loc[['general', 'refined', 'core']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check affinity distributions\n",
    "\n",
    "grid = sns.FacetGrid(affinity_data[affinity_data['include']], row='set', row_order=['general', 'refined', 'core'],\n",
    "                     size=3, aspect=3)\n",
    "grid.map(sns.distplot, '-logKd/Ki');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_data[['pdbid']].to_csv('pdb_custom.ids', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_data[['pdbid', '-logKd/Ki', 'set']].to_csv('affinity_data_cleaned_custom.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = {'general': 'general-set-except-refined', 'refined': 'refined-set', 'core': 'refined-set'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%bash \n",
    "\n",
    "# # Prepare pockets with UCSF Chimera - pybel sometimes fails to calculate the charges.\n",
    "# # Even if Chimera fails to calculate several charges (mostly for non-standard residues),\n",
    "# # it returns charges for other residues.\n",
    "\n",
    "# for pdbfile in /scratch/kanakala.ganesh/PDBBIND/v2019-other-PL/*/*_pocket.pdb; do\n",
    "#     mol2file=${pdbfile%pdb}mol2\n",
    "# #     echo $m\n",
    "# # ol2file \n",
    "# #     if [[ ! -f $mol2file ]]; then\n",
    "#         echo $mol2file\n",
    "#         echo -e \"open $pdbfile \\n addh \\n addcharge \\n write format mol2 0 tmp.mol2 \\n stop\" | bash /home2/jai/.local/UCSF-Chimera64-1.15rc/bin/chimera --nogui\n",
    "#         # Do not use TIP3P atom types, pybel cannot read them\n",
    "#         sed 's/H\\.t3p/H    /' tmp.mol2 | sed 's/O\\.t3p/O\\.3  /' > $mol2file\n",
    "# #     fi\n",
    "# done \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%bash -s $path\n",
    "\n",
    "# # Prepare pockets with UCSF Chimera - pybel sometimes fails to calculate the charges.\n",
    "# # Even if Chimera fails to calculate several charges (mostly for non-standard residues),\n",
    "# # it returns charges for other residues.\n",
    "\n",
    "# path=$1\n",
    "\n",
    "# for dataset in general-set-except-refined refined-set; do\n",
    "#     echo $dataset\n",
    "#     for pdbfile in /scratch/kanakala.ganesh/2019/coreset/*/*_pocket.pdb; do\n",
    "#         mol2file=${pdbfile%pdb}mol2\n",
    "#         if [[ ! -e $mol2file ]]; then\n",
    "#             echo -e \"open $pdbfile \\n addh \\n addcharge \\n write format mol2 0 tmp.mol2 \\n stop\" | chimera --nogui\n",
    "#             # Do not use TIP3P atom types, pybel cannot read them\n",
    "#             sed 's/H\\.t3p/H    /' tmp.mol2 | sed 's/O\\.t3p/O\\.3  /' > $mol2file\n",
    "#         fi\n",
    "#     done \n",
    "# done > chimera_rw.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "featurizer = Featurizer()\n",
    "charge_idx = featurizer.FEATURE_NAMES.index('partialcharge')\n",
    "\n",
    "path = '/scratch/kanakala.ganesh/PDBBIND/v2019-other-PL/'\n",
    "ds_path = ''\n",
    "# ds_path =\"refined-set\"\n",
    "# ds_path= \"v2019-other-PL\"\n",
    "data = affinity_data[affinity_data['set']=='general']    \n",
    "dataset_name='general'\n",
    "i=0\n",
    "pocket_failed=[]\n",
    "ligand_failed=[]\n",
    "count = 0\n",
    "with h5py.File('%s/%s.hdf' % (path, dataset_name), 'w') as f:\n",
    "    for _, row in data.iterrows():\n",
    "        \n",
    "        if row['pdbid'] in general_list:\n",
    "            \n",
    "            name = row['pdbid']\n",
    "            affinity = row['-logKd/Ki']\n",
    "            include = row['include']\n",
    "            ligand = next(pybel.readfile('mol2', '%s/%s/%s/%s_ligand.mol2' % (path, ds_path, name, name)))\n",
    "            # do not add the hydrogens! they are in the strucutre and it would reset the charges\n",
    "\n",
    "            try:\n",
    "                pocket = next(pybel.readfile('mol2', '%s/%s/%s/%s_pocket.mol2' % (path, ds_path, name, name)))\n",
    "                # do not add the hydrogens! they were already added in chimera and it would reset the charges\n",
    "            except:\n",
    "                warnings.warn('no pocket for %s (%s set)' % (name, dataset_name))\n",
    "                print('count',count)\n",
    "                count +=1 \n",
    "                continue\n",
    "\n",
    "            ligand_coords, ligand_features = featurizer.get_features(ligand, molcode=1)\n",
    "            if not (ligand_features[:, charge_idx] != 0).any():\n",
    "                ligand_failed.append(i)\n",
    "\n",
    "            pocket_coords, pocket_features = featurizer.get_features(pocket, molcode=-1)\n",
    "            if not (pocket_features[:, charge_idx] != 0).any():\n",
    "                pocket_failed.append(i)\n",
    "\n",
    "            centroid = ligand_coords.mean(axis=0)\n",
    "            ligand_coords -= centroid\n",
    "            pocket_coords -= centroid\n",
    "\n",
    "            data = np.concatenate((np.concatenate((ligand_coords, pocket_coords)),\n",
    "                                   np.concatenate((ligand_features, pocket_features))), axis=1)\n",
    "    #         print(include)\n",
    "\n",
    "            if row['include']:\n",
    "                dataset = f.create_dataset(name, data=data, shape=data.shape, dtype='float32', compression='lzf')\n",
    "                dataset.attrs['affinity'] = affinity\n",
    "                i += 1\n",
    "                print(i)\n",
    "#             break\n",
    "print('prepared', i, 'complexes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ligand_failed), len(pocket_failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featurizer = Featurizer()\n",
    "# charge_idx = featurizer.FEATURE_NAMES.index('partialcharge')\n",
    "\n",
    "# with h5py.File('%s/core2013.hdf' % path, 'w') as g:\n",
    "#     j = 0\n",
    "\n",
    "#     for dataset_name, data in affinity_data.groupby('set'):\n",
    "\n",
    "#         print(dataset_name, 'set')\n",
    "#         i = 0\n",
    "#         ds_path = dataset_path[dataset_name]\n",
    "\n",
    "\n",
    "#         with h5py.File('%s/%s.hdf' % (path, dataset_name), 'w') as f:\n",
    "#             for _, row in data.iterrows():\n",
    "\n",
    "#                 name = row['pdbid']\n",
    "#                 affinity = row['-logKd/Ki']\n",
    "\n",
    "#                 ligand = next(pybel.readfile('mol2', '%s/%s/%s/%s_ligand.mol2' % (path, ds_path, name, name)))\n",
    "#                 # do not add the hydrogens! they are in the strucutre and it would reset the charges\n",
    "\n",
    "#                 try:\n",
    "#                     pocket = next(pybel.readfile('mol2', '%s/%s/%s/%s_pocket.mol2' % (path, ds_path, name, name)))\n",
    "#                     # do not add the hydrogens! they were already added in chimera and it would reset the charges\n",
    "#                 except:\n",
    "#                     warnings.warn('no pocket for %s (%s set)' % (name, dataset_name))\n",
    "#                     continue\n",
    "\n",
    "#                 ligand_coords, ligand_features = featurizer.get_features(ligand, molcode=1)\n",
    "#                 assert (ligand_features[:, charge_idx] != 0).any()\n",
    "#                 pocket_coords, pocket_features = featurizer.get_features(pocket, molcode=-1)\n",
    "#                 assert (pocket_features[:, charge_idx] != 0).any() \n",
    "\n",
    "#                 centroid = ligand_coords.mean(axis=0)\n",
    "#                 ligand_coords -= centroid\n",
    "#                 pocket_coords -= centroid\n",
    "\n",
    "#                 data = np.concatenate((np.concatenate((ligand_coords, pocket_coords)),\n",
    "#                                        np.concatenate((ligand_features, pocket_features))), axis=1)\n",
    "\n",
    "#                 if row['include']:\n",
    "#                     dataset = f.create_dataset(name, data=data, shape=data.shape, dtype='float32', compression='lzf')\n",
    "#                     dataset.attrs['affinity'] = affinity\n",
    "#                     i += 1\n",
    "#                 else:\n",
    "#                     dataset = g.create_dataset(name, data=data, shape=data.shape, dtype='float32', compression='lzf')\n",
    "#                     dataset.attrs['affinity'] = affinity\n",
    "#                     j += 1\n",
    "\n",
    "#         print('prepared', i, 'complexes')\n",
    "#     print('excluded', j, 'complexes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./DATA/general.hdf', 'r') as g, \\\n",
    "     h5py.File('./DATA/refined.hdf', 'r') as r, \\\n",
    "     h5py.File('./DATA/full.hdf', 'w') as f:\n",
    "    for name in g:\n",
    "        dataset= f.create_dataset(name, data=g[name])\n",
    "        dataset.attrs['affinity'] = g[name].attrs['affinity']\n",
    "    for name in r:\n",
    "        dataset= f.create_dataset(name, data=r[name])\n",
    "        dataset.attrs['affinity'] = r[name].attrs['affinity']\n",
    "\n",
    "f.close()\n",
    "g.close()\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold0 = pd.read_csv('../FOLDS/full_pk_cv_test0.types', sep=' ')\n",
    "fold1 = pd.read_csv('../FOLDS/full_pk_cv_test1.types', sep=' ')\n",
    "fold2 = pd.read_csv('../FOLDS/full_pk_cv_test2.types', sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_ids = list(fold0['pdbid'].values) + list(fold1['pdbid'].values) + list(fold2['pdbid'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../folds_using_0.30_thresh_pocketmatch.pkl', 'rb') as f:\n",
    "    folds = pickle.load(f)\n",
    "    \n",
    "# for fold in folds:\n",
    "#     for pdbid in fold:\n",
    "#         if pdbid not in fold_ids:\n",
    "#             fold.remove(pdbid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(len(i)) for i in folds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./DATA/full.hdf', 'r') as f, \\\n",
    "     h5py.File('./DATA/training_set.hdf', 'w') as t1, \\\n",
    "     h5py.File('./DATA/test_set.hdf', 'w') as t2  :\n",
    "     for name in f:\n",
    "        if name in folds[0] or name in folds[1]:\n",
    "            data = t1.create_dataset(name, data=f[name])\n",
    "            data.attrs['affinity'] = f[name].attrs['affinity']\n",
    "        else:\n",
    "            data = t2.create_dataset(name, data=f[name])\n",
    "            data.attrs['affinity'] = f[name].attrs['affinity']\n",
    "        \n",
    "t1.close()\n",
    "t2.close()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
